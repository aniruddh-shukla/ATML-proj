{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oishi_ATIML_final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TSyHK_bQO8J7",
        "3o7_P423OVhu",
        "c6VpZbNtbU9u",
        "A3btQC87c3Wa",
        "Y0mlz46-Cqql",
        "m0sKlFk8QFlV",
        "MwHFHrhoCwkV",
        "9t0BxFtD83u6",
        "JoJTgi8BQFlY",
        "8FN27lfqgezx",
        "bdeh5mdFhEeb",
        "9pgFtYFWgi0F"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSyHK_bQO8J7"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "mbp72iRxX8Q7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b8b70f-432c-4540-987a-0adfd72720cc"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U textstat\n",
        "!pip install textblob\n",
        "!pip install --upgrade scikit-learn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.0.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.36.2)\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.7/dist-packages (0.7.1)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.7/dist-packages (from textstat) (0.10.0)\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o7_P423OVhu"
      },
      "source": [
        "# All Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JIYuoyc-ZOPW",
        "outputId": "bad58284-2c7c-4429-e4a8-da80c39b8d21"
      },
      "source": [
        "import re\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import textstat\n",
        "import string\n",
        "import nltk\n",
        "from copy import deepcopy\n",
        "from textblob import TextBlob\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, KFold, StratifiedKFold, ShuffleSplit \n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import preprocessing\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn import metrics\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier, LabelSpreading\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6VpZbNtbU9u"
      },
      "source": [
        "# Fetch Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXxJPkRqZ4jt"
      },
      "source": [
        "news_group = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), shuffle=True)\n",
        "train_X, test_X, train_y, test_y = train_test_split(news_group.data, news_group.target, test_size=0.2, stratify=news_group.target)\n",
        "#news_group_data = news_group.data\n",
        "#news_group_target_names = news_group.target_names\n",
        "#news_group_target = news_group.target\n",
        "\n",
        "#news_group_test = fetch_20newsgroups(subset='test')\n",
        "#news_group_test_data = news_group_test.data\n",
        "#news_group_test_target_names = news_group_test.target_names\n",
        "#news_group_test_target = news_group_test.target"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb5VqcGucNSC"
      },
      "source": [
        "# Convert to Pandas DF and Random Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2G1qbWGcFzA"
      },
      "source": [
        "news_df = pd.DataFrame({'news': train_X, \n",
        "                        'class': train_y})\n",
        "\n",
        "#news_sampled = news_df.sample(2000)\n",
        "#news_sampled.reset_index(drop=True, inplace=True)\n",
        "\n",
        "news_df_test = pd.DataFrame({'news': test_X, \n",
        "                        'class': test_y})\n",
        "\n",
        "#news_sampled_test = news_df_test.sample(400)\n",
        "#news_sampled_test.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3btQC87c3Wa"
      },
      "source": [
        "# Cleaning Text\n",
        "\n",
        "*   Cleaning\n",
        "*   Removing stop words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI10L4oGDe-M"
      },
      "source": [
        "class Cleaner():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stop_words = stopwords.words('english')\n",
        "        self.re_url = re.compile(r'(?:http|ftp|https)://(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?')\n",
        "        self.re_email = re.compile('(?:[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])')\n",
        "\n",
        "    def clean_news(self, text):\n",
        "        text = re.sub(r'(From:\\s+[^\\n]+\\n)', '', text) # remove From\n",
        "        text = re.sub(r'(Subject:)', '', text) # remove the word \"Subject:\"\"\n",
        "        text = text.lower() # Convert to lowerCase\n",
        "        text = text.strip() # Strip terminal spaces\n",
        "        text = re.sub(self.re_url, '', text)\n",
        "        text = re.sub(self.re_email, '', text)       \n",
        "        text = re.sub(r'\\s+\\w{1}\\s+', ' ', text) #remove single char\n",
        "        #text = text.replace('\\n',' ')\n",
        "        text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) # punctuations\n",
        "        text = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', ' ', text) # remove pure digits\n",
        "        text = re.sub(r'(\\s+)', ' ', text) # replace >1 whitespaces with single space\n",
        "\n",
        "        return text\n",
        "\n",
        "    def removeStopWords(self, text):\n",
        "        \n",
        "        x = text.split(' ')\n",
        "        for word in x:\n",
        "            if(word in self.stop_words):\n",
        "                x = list(filter((word).__ne__, x))\n",
        "        return ' '.join(x)\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "    def transform(self, data):\n",
        "        cleaner = Cleaner()\n",
        "        \n",
        "        data_array = []\n",
        "        for d in data:\n",
        "            s = cleaner.clean_news(d)\n",
        "            w = cleaner.removeStopWords(s)\n",
        "            data_array.append(w)\n",
        "        return data_array "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0mlz46-Cqql"
      },
      "source": [
        "# BOW Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqHpexX0Ju8b"
      },
      "source": [
        "class BOWVectorizer():\n",
        "    def __init__(self):\n",
        "        self.vectorize = None\n",
        "    def fit(self, x, y=None):\n",
        "        bowvec = TfidfVectorizer()\n",
        "        bowvec.fit(x)\n",
        "        self.vectorize = bowvec\n",
        "        return self.vectorize\n",
        "    \n",
        "    def transform(self, data):\n",
        "        x = self.vectorize.transform(data)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0sKlFk8QFlV"
      },
      "source": [
        "# POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-rv3AqYR0xp"
      },
      "source": [
        "class POSVectorizer():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def creatingPOSTags(self, x):\n",
        "             \n",
        "        pos_family = {'NOUN' : ['NN','NNS','NNP','NNPS'],\n",
        "                    'PRON' : ['PRP','PRP$','WP','WP$'], \n",
        "                    'VERB' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "                    'ADJ'  : ['JJ','JJR','JJS'],\n",
        "                    'ADV'  : ['RB','RBR','RBS','WRB']\n",
        "                    }\n",
        "            \n",
        "        count_pos = {'NOUN':0,'PRON':0,'VERB':0,'ADJ':0,'ADV':0}\n",
        "        \n",
        "        blob  = TextBlob(x) #converts sentences to tokens\n",
        "        for tuple in blob.tags: #blob tags contains term and its pos\n",
        "            #print(tuple)\n",
        "            pos = list(tuple)[1]\n",
        "            if pos in pos_family['NOUN']:\n",
        "                count_pos['NOUN'] = count_pos.get('NOUN')+1\n",
        "            elif pos in pos_family['PRON']:\n",
        "                count_pos['PRON'] = count_pos.get('PRON')+1\n",
        "            elif pos in pos_family['VERB']:\n",
        "                count_pos['VERB'] = count_pos.get('VERB')+1\n",
        "            elif pos in pos_family['ADJ']:\n",
        "                count_pos['ADJ'] = count_pos.get('ADJ')+1\n",
        "            elif pos in pos_family['ADV']:\n",
        "                count_pos['ADV'] = count_pos.get('ADV')+1\n",
        "        return count_pos \n",
        "    \n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "    def transform(self, data):\n",
        "        posVector = POSVectorizer()\n",
        "        pos_vect = []\n",
        "        for d in data:\n",
        "            pos_vect.append(posVector.creatingPOSTags(d))\n",
        "        return pos_vect"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwHFHrhoCwkV"
      },
      "source": [
        "# Convert toArray()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN4pqe_mEnmh"
      },
      "source": [
        "class ToArray():\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.toarray()\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):        \n",
        "        return self"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKc70oP3C2PG"
      },
      "source": [
        "# Creating Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L32vijlFEwMh"
      },
      "source": [
        "bow_transformer = Pipeline(\n",
        "    steps=[\n",
        "        (\"cleaner\", Cleaner()),\n",
        "        (\"bow\", BOWVectorizer()),\n",
        "        (\"toarray\", ToArray()), #converting toarray since minmax can't handle sparce matrix\n",
        "        (\"scale\", preprocessing.MinMaxScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "pos_transformer = Pipeline(\n",
        "    steps=[\n",
        "        (\"cleaner\", Cleaner()),\n",
        "        (\"pos\", POSVectorizer()),\n",
        "        (\"dict_vect\", DictVectorizer()),\n",
        "        (\"toarray\", ToArray()), #converting toarray since minmax can't handle sparce matrix\n",
        "        (\"scale\", preprocessing.MinMaxScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "combined_features = FeatureUnion(\n",
        "    transformer_list=[\n",
        "        (\"bow\", bow_transformer),\n",
        "        (\"pos\", pos_transformer),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def fitFinalPipeline(classifier, data_X, data_Y, unlabeled=None, n_folds=5):\n",
        "    final_pipeline = Pipeline(\n",
        "        steps=[\n",
        "            (\"combined_features\", combined_features),\n",
        "            ('chi',  SelectKBest(chi2, k=20000)),\n",
        "            (\"classifier\", classifier),\n",
        "        ]\n",
        "    )\n",
        "    #print(final_pipeline.steps)\n",
        "\n",
        "    kf = StratifiedKFold(n_splits=n_folds)\n",
        "    train_scores = list() # training accuracy\n",
        "    avg_accuracy = 0\n",
        "    \n",
        "    #original_clf = deepcopy(final_pipeline)\n",
        "    \n",
        "    for train_ids, valid_ids in kf.split(data_X, data_Y):\n",
        "        #cv_clf = deepcopy(original_clf)\n",
        "        train_X, train_y, valid_X, valid_y = data_X[train_ids], data_Y[train_ids], data_X[valid_ids], data_Y[valid_ids]\n",
        "        \n",
        "        if unlabeled==None:\n",
        "            final_pipeline.fit(train_X, train_y)            \n",
        "        else:            \n",
        "            final_pipeline.fit(train_X, train_y, unlabeled)\n",
        "\n",
        "        pred = final_pipeline.predict(valid_X)\n",
        "        \n",
        "        scores = metrics.accuracy_score(valid_y, pred)\n",
        "        train_scores.append(scores)\n",
        "        avg_accuracy += scores\n",
        "    \n",
        "    print(\"Average training accuracy: %0.3f\" % (avg_accuracy/n_folds))\n",
        "    #final_pipeline.fit(X_train, Y_train)\n",
        "    #y_pred = final_pipeline.predict(X_test)\n",
        "    #cr = classification_report(Y_test, y_pred)\n",
        "    #print(cr)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKIDhioaCaw3"
      },
      "source": [
        "# RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYIT8jWXE7Tu",
        "outputId": "9ea5d1eb-9be4-4ea6-e545-642a81239d84"
      },
      "source": [
        "fitFinalPipeline (RandomForestClassifier(), news_df['news'], news_df['class'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average training accuracy: 0.628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t0BxFtD83u6"
      },
      "source": [
        "# EM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiwyS7ZIhKcD"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from copy import deepcopy\n",
        "from scipy.sparse import csr_matrix, vstack\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from scipy.linalg import get_blas_funcs\n",
        "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
        "\n",
        "class Semi_EM_MultinomialNB():\n",
        "    \"\"\"\n",
        "    Naive Bayes classifier for multinomial models for semi-supervised learning.\n",
        "    \n",
        "    Use both labeled and unlabeled data to train NB classifier, update parameters\n",
        "    using unlabeled data, and all data to evaluate performance of classifier. Optimize\n",
        "    classifier using Expectation-Maximization algorithm.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None, max_iter=30, tol=1e-6, print_log_lkh=True):\n",
        "        self.alpha = alpha\n",
        "        self.fit_prior = fit_prior\n",
        "        self.class_prior = class_prior\n",
        "        self.clf = MultinomialNB(alpha=self.alpha, fit_prior=self.fit_prior, class_prior=self.class_prior)\n",
        "        self.log_lkh = -np.inf # log likelihood\n",
        "        self.max_iter = max_iter # max number of EM iterations\n",
        "        self.tol = tol # tolerance of log likelihood increment\n",
        "        self.feature_log_prob_ = np.array([]) # Empirical log probability of features given a class, P(x_i|y).\n",
        "        self.coef_ = np.array([]) # Mirrors feature_log_prob_ for interpreting MultinomialNB as a linear model.\n",
        "        self.print_log_lkh = print_log_lkh # if True, print log likelihood during EM iterations\n",
        "\n",
        "    def fit(self, X_l, y_l, X_u):\n",
        "        \"\"\"\n",
        "        Initialize the parameter using labeled data only.\n",
        "        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.\n",
        "        \"\"\"\n",
        "        n_ul_docs = X_u.shape[0] # number of unlabeled samples\n",
        "        n_l_docs = X_l.shape[0] # number of labeled samples\n",
        "        # initialization (n_docs = n_ul_docs)\n",
        "        clf = deepcopy(self.clf)# build new copy of classifier\n",
        "        clf.fit(X_l, y_l) # use labeled data only to initialize classifier parameters\n",
        "        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration\n",
        "        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
        "        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]\n",
        "        lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.T.toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
        "        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T.toarray()) \n",
        "        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
        "        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
        "        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]\n",
        "        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]\n",
        "        expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
        "        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
        "        self.clf = deepcopy(clf)\n",
        "        self.log_lkh = expectation\n",
        "        if self.print_log_lkh:\n",
        "            print(\"Initial expected log likelihood = %0.3f\\n\" % expectation)\n",
        "        # Loop until log likelihood does not improve\n",
        "        iter_count = 0 # count EM iteration\n",
        "        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):\n",
        "        # while (iter_count<self.max_iter):\n",
        "            iter_count += 1\n",
        "            if self.print_log_lkh:\n",
        "                print(\"EM iteration #%d\" % iter_count) # debug\n",
        "            # E-step: Estimate class membership of unlabeled documents\n",
        "            y_u = clf.predict(X_u)\n",
        "            # M-step: Re-estimate classifier parameters\n",
        "            X = vstack([X_l, X_u])\n",
        "            y = np.concatenate((y_l, y_u), axis=0)\n",
        "            clf.fit(X, y)\n",
        "            # check convergence: update log likelihood\n",
        "            p_c_d = clf.predict_proba(X_u)\n",
        "            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
        "            b_w_d = (X_u > 0) # words in each document\n",
        "            lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.transpose().toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
        "            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose().toarray()) \n",
        "            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
        "            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
        "            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]\n",
        "            expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
        "            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
        "            if self.print_log_lkh:\n",
        "                print(\"\\tExpected log likelihood = %0.3f\" % expectation)\n",
        "            if (expectation-self.log_lkh >= self.tol):\n",
        "                prev_log_lkh = self.log_lkh\n",
        "                self.log_lkh = expectation\n",
        "                self.clf = deepcopy(clf)\n",
        "            else:\n",
        "                break\n",
        "        self.feature_log_prob_ = self.clf.feature_log_prob_\n",
        "        self.coef_ = self.clf.coef_\n",
        "        return self\n",
        "\n",
        "    def fit_with_clustering(self, X_l, y_l, X_u, y_u=None):\n",
        "        \"\"\"\n",
        "        Initialize the parameter using both labeled and unlabeled data.\n",
        "        The classes of unlabeled data are assigned using similarity with labeled data.\n",
        "        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.\n",
        "        The label propagation can only use dense matrix, so it is quite time consuming.\n",
        "        \"\"\"\n",
        "        n_ul_docs = X_u.shape[0] # number of unlabeled samples\n",
        "        n_l_docs = X_l.shape[0] # number of labeled samples\n",
        "        # initialization (n_docs = n_ul_docs): \n",
        "        # assign class to unlabeled data using similarity with labeled data if y_u is not given\n",
        "        if (y_u==None):\n",
        "            label_prop_model = LabelSpreading(kernel='rbf', max_iter=5, n_jobs=-1)\n",
        "            y_u = np.array([-1.0]*n_ul_docs)\n",
        "            X = vstack([X_l, X_u])\n",
        "            y = np.concatenate((y_l, y_u), axis=0)\n",
        "            label_prop_model.fit(X.toarray(), y)\n",
        "            y_u = label_prop_model.predict(X_u.toarray())\n",
        "        y = np.concatenate((y_l, y_u), axis=0)\n",
        "        clf = deepcopy(self.clf)# build new copy of classifier\n",
        "        clf.fit(X, y) # use labeled data only to initialize classifier parameters\n",
        "        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration\n",
        "        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
        "        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]\n",
        "        lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.T.toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
        "        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T.toarray()) \n",
        "        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
        "        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
        "        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]\n",
        "        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]\n",
        "        expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
        "        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
        "        self.clf = deepcopy(clf)\n",
        "        self.log_lkh = expectation\n",
        "        if self.print_log_lkh:\n",
        "            print(\"Initial expected log likelihood = %0.3f\\n\" % expectation)\n",
        "        # Loop until log likelihood does not improve\n",
        "        iter_count = 0 # count EM iteration\n",
        "        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):\n",
        "        # while (iter_count<self.max_iter):\n",
        "            iter_count += 1\n",
        "            if self.print_log_lkh:\n",
        "                print(\"EM iteration #%d\" % iter_count) # debug\n",
        "            # E-step: Estimate class membership of unlabeled documents\n",
        "            y_u = clf.predict(X_u)\n",
        "            # M-step: Re-estimate classifier parameters\n",
        "            X = vstack([X_l, X_u])\n",
        "            y = np.concatenate((y_l, y_u), axis=0)\n",
        "            clf.fit(X, y)\n",
        "            # check convergence: update log likelihood\n",
        "            p_c_d = clf.predict_proba(X_u)\n",
        "            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
        "            b_w_d = (X_u > 0) # words in each document\n",
        "            lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.transpose().toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
        "            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose().toarray()) \n",
        "            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
        "            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
        "            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]\n",
        "            expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
        "            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
        "            if self.print_log_lkh:\n",
        "                print(\"\\tExpected log likelihood = %0.3f\" % expectation)\n",
        "            if (expectation-self.log_lkh >= self.tol):\n",
        "                prev_log_lkh = self.log_lkh\n",
        "                self.log_lkh = expectation\n",
        "                self.clf = deepcopy(clf)\n",
        "            else:\n",
        "                break\n",
        "        self.feature_log_prob_ = self.clf.feature_log_prob_\n",
        "        self.coef_ = self.clf.coef_\n",
        "        return self\n",
        "\n",
        "    def partial_fit(self, X_l, y_l, X_u=np.array([])):\n",
        "        \"\"\"\n",
        "        Initialize the parameter using labeled data only.\n",
        "        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.\n",
        "        This function can only be used after fit()\n",
        "        \"\"\"\n",
        "        n_ul_docs = X_u.shape[0] # number of unlabeled samples\n",
        "        n_l_docs = X_l.shape[0] # number of labeled samples\n",
        "        # initialization (n_docs = n_ul_docs)\n",
        "        clf = deepcopy(self.clf)# build new copy of classifier\n",
        "        clf.partial_fit(X_l, y_l) # use labeled data only to initialize classifier parameters\n",
        "        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration\n",
        "        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
        "        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]\n",
        "        lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.T.toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
        "        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T.toarray()) \n",
        "        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
        "        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
        "        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]\n",
        "        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]\n",
        "        expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
        "        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
        "        self.clf = deepcopy(clf)\n",
        "        self.log_lkh = expectation\n",
        "        print(\"Initial expected log likelihood = %0.3f\\n\" % expectation)\n",
        "        # Loop until log likelihood does not improve\n",
        "        iter_count = 0 # count EM iteration\n",
        "        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):\n",
        "        # while (iter_count<self.max_iter):\n",
        "            iter_count += 1\n",
        "            print(\"EM iteration #%d\" % iter_count) # debug\n",
        "            # E-step: Estimate class membership of unlabeled documents\n",
        "            y_u = clf.predict(X_u)\n",
        "            # M-step: Re-estimate classifier parameters\n",
        "            X = vstack([X_l, X_u])\n",
        "            y = np.concatenate((y_l, y_u), axis=0)\n",
        "            clf.partial_fit(X, y)\n",
        "            # check convergence: update log likelihood\n",
        "            p_c_d = clf.predict_proba(X_u)\n",
        "            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
        "            b_w_d = (X_u > 0) # words in each document\n",
        "            lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.transpose().toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
        "            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose().toarray()) \n",
        "            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
        "            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
        "            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]\n",
        "            expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
        "            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
        "            print(\"\\tExpected log likelihood = %0.3f\" % expectation)\n",
        "            if (expectation-self.log_lkh >= self.tol):\n",
        "                prev_log_lkh = self.log_lkh\n",
        "                self.log_lkh = expectation\n",
        "                self.clf = deepcopy(clf)\n",
        "            else:\n",
        "                break\n",
        "        self.feature_log_prob_ = self.clf.feature_log_prob_\n",
        "        self.coef_ = self.clf.coef_\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.clf.predict(X)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return self.clf.score(X, y)\n",
        "\n",
        "    def get_params(deep=True):\n",
        "        return self.clf.get_params(deep)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.clf.__str__()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi-Up6na8_mO"
      },
      "source": [
        "# EM Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UObdtpJ0JmGs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "934674d1-ecd2-4e77-eca5-a249d2966e31"
      },
      "source": [
        "#X_l, X_u, y_l, y_u = train_test_split(train_X, train_y, test_size=10000, stratify=train_y)\n",
        "experiments = np.logspace(2.3, 3.7, num=20, base=10, dtype='int')\n",
        "for n_l_docs in experiments:\n",
        "    em_nb_clf = Semi_EM_MultinomialNB(alpha=1e-2, tol=100, print_log_lkh=False) # semi supervised EM based Naive Bayes classifier\n",
        "    fitFinalPipeline (em_nb_clf, news_df['news'][:n_l_docs], news_df['class'][:n_l_docs], news_df_test['news'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-51fc8211d5d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_l_docs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mem_nb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSemi_EM_MultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_log_lkh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# semi supervised EM based Naive Bayes classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfitFinalPipeline\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mem_nb_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'news'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_l_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_l_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_df_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'news'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-602111977f47>\u001b[0m in \u001b[0;36mfitFinalPipeline\u001b[0;34m(classifier, data_X, data_Y, unlabeled, n_folds)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0munlabeled\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mfinal_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoJTgi8BQFlY"
      },
      "source": [
        "# SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM7qZgFxtAIR"
      },
      "source": [
        "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log')\n",
        "\n",
        "fitFinalPipeline (SGDClassifier(**sdg_params), news_df['news'], news_df['class'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FN27lfqgezx"
      },
      "source": [
        "# Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqRvFeunaFkK",
        "outputId": "7f02e329-72cb-4dc1-a4dd-ad2fd43965bf"
      },
      "source": [
        "y_mask = np.random.rand(len(news_sampled['class'])) < 0.2\n",
        "y_masked_class = news_sampled\n",
        "y_masked_class['class'][~y_mask] = -1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdeh5mdFhEeb"
      },
      "source": [
        "# LabelSpreading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq3aPO-LhBz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909ff3ac-de09-40df-a8a2-3f97fa02d19d"
      },
      "source": [
        "fitFinalPipeline (LabelSpreading(gamma=0.25, max_iter=50), news_sampled['news'], y_masked_class['class'], news_sampled_test['news'], news_sampled_test['class'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/semi_supervised/_label_propagation.py:292: ConvergenceWarning: max_iter=5 was reached without convergence.\n",
            "  category=ConvergenceWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.00      0.00      0.00        17\n",
            "           2       0.06      1.00      0.11        22\n",
            "           3       1.00      0.05      0.10        19\n",
            "           4       0.00      0.00      0.00        21\n",
            "           5       1.00      0.04      0.08        25\n",
            "           6       0.00      0.00      0.00        19\n",
            "           7       0.00      0.00      0.00        21\n",
            "           8       0.00      0.00      0.00        21\n",
            "           9       0.00      0.00      0.00        18\n",
            "          10       0.00      0.00      0.00        22\n",
            "          11       0.00      0.00      0.00        10\n",
            "          12       0.00      0.00      0.00        23\n",
            "          13       0.00      0.00      0.00        20\n",
            "          14       0.00      0.00      0.00        30\n",
            "          15       0.00      0.00      0.00        23\n",
            "          16       0.00      0.00      0.00        18\n",
            "          17       0.00      0.00      0.00        24\n",
            "          18       0.00      0.00      0.00        19\n",
            "          19       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           0.06       400\n",
            "   macro avg       0.10      0.05      0.01       400\n",
            "weighted avg       0.11      0.06      0.02       400\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/semi_supervised/_label_propagation.py:205: RuntimeWarning: invalid value encountered in true_divide\n",
            "  probabilities /= normalizer\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pgFtYFWgi0F"
      },
      "source": [
        "# SelfTrainingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhJG2zvsXij3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675a6cd1-c454-4dcc-b03f-abad7b5adfcb"
      },
      "source": [
        "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log')\n",
        "fitFinalPipeline (SelfTrainingClassifier(SGDClassifier(**sdg_params), verbose=True), news_sampled['news'], y_masked_class['class'], news_sampled_test['news'], news_sampled_test['class'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End of iteration 1, added 1458 new labels.\n",
            "End of iteration 2, added 127 new labels.\n",
            "End of iteration 3, added 16 new labels.\n",
            "End of iteration 4, added 1 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.50      0.50        18\n",
            "           1       0.83      0.29      0.43        17\n",
            "           2       0.52      0.64      0.57        22\n",
            "           3       1.00      0.11      0.19        19\n",
            "           4       0.36      0.19      0.25        21\n",
            "           5       0.43      0.64      0.52        25\n",
            "           6       0.60      0.63      0.62        19\n",
            "           7       0.50      0.52      0.51        21\n",
            "           8       0.68      0.62      0.65        21\n",
            "           9       0.62      0.56      0.59        18\n",
            "          10       0.54      0.68      0.60        22\n",
            "          11       0.36      0.50      0.42        10\n",
            "          12       0.75      0.13      0.22        23\n",
            "          13       0.13      0.65      0.22        20\n",
            "          14       0.59      0.33      0.43        30\n",
            "          15       0.55      0.26      0.35        23\n",
            "          16       0.43      0.17      0.24        18\n",
            "          17       0.41      0.50      0.45        24\n",
            "          18       0.60      0.32      0.41        19\n",
            "          19       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           0.42       400\n",
            "   macro avg       0.52      0.41      0.41       400\n",
            "weighted avg       0.54      0.42      0.42       400\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}