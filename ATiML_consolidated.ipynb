{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATiML_consolidated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Tg4EfTloYYkZZYXkOXzt8NK-Ns9tfCj6",
      "authorship_tag": "ABX9TyMLdWrDbNmpjrjd2JM/Ou1B",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniruddh-shukla/ATML-proj/blob/main/ATiML_consolidated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma6iYpLnS7cB",
        "outputId": "7852f9c6-bd05-4d29-d910-5ea3c5d8851e"
      },
      "source": [
        "!pip install auto-sklearn\n",
        "!pip install gensim\n",
        "!pip install scikit-learn==0.24.2\n",
        "!pip install dask distributed --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting auto-sklearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/1b/9249f7d4498cbdb0130352a838e02ca7d7bebbcacde74d1f4f26a5d9202b/auto-sklearn-0.12.6.tar.gz (6.1MB)\n",
            "\u001b[K     |████████████████████████████████| 6.1MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (56.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.0.1)\n",
            "Collecting scikit-learn<0.25.0,>=0.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (2.12.0)\n",
            "Collecting distributed>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/be/e150dfcdceb22ac1a15ed6ab03268bd760b59cce6b6d4e584454efd8ca8c/distributed-2021.5.0-py3-none-any.whl (699kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 23.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (3.13)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.1.5)\n",
            "Collecting liac-arff\n",
            "  Downloading https://files.pythonhosted.org/packages/6e/43/73944aa5ad2b3185c0f0ba0ee6f73277f2eb51782ca6ccf3e6793caf209a/liac-arff-2.5.0.tar.gz\n",
            "Collecting ConfigSpace<0.5,>=0.4.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/19/726bcb262949ec28c71ba571bfb1a0006f985e7141a83cff34c733c75739/ConfigSpace-0.4.19-cp37-cp37m-manylinux2014_x86_64.whl (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 35.6MB/s \n",
            "\u001b[?25hCollecting pynisher>=0.6.3\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/39/edac9acf3bd245ecf475151014cce3652c25ca3c2352eac725502cfce6ea/pynisher-0.6.4.tar.gz\n",
            "Collecting pyrfr<0.9,>=0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/1a/56b630c949e942d12f4ad5f4fd240c1cf2e1260e5126190b171ca2aa9199/pyrfr-0.8.2-cp37-cp37m-manylinux2014_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 42.8MB/s \n",
            "\u001b[?25hCollecting smac<0.14,>=0.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/f2/8ea040eaa2253a3606472b08d9c2a23be1a177c0c19e236a2b3222c0fd78/smac-0.13.1.tar.gz (258kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 54.3MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (1.7.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: tornado>=5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (5.1.1)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (0.11.1)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (5.4.8)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (1.0.2)\n",
            "Collecting cloudpickle>=1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (7.1.2)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (2.0.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn) (2.8.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn) (2.4.7)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn) (0.29.23)\n",
            "Collecting lazy_import\n",
            "  Downloading https://files.pythonhosted.org/packages/44/2e/5378f9b9cbc893826c2ecb022646c97ece9efbaad351adf89425fff33990/lazy_import-0.2.2.tar.gz\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.2.0->auto-sklearn) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->auto-sklearn) (1.15.0)\n",
            "Building wheels for collected packages: auto-sklearn, liac-arff, pynisher, smac, lazy-import\n",
            "  Building wheel for auto-sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for auto-sklearn: filename=auto_sklearn-0.12.6-cp37-none-any.whl size=6370105 sha256=cbfe4197a4a61c83468462e8de6038bb050a39d94d9ccc9500b718f0f804933f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/c8/1f/3a6d11c1e156bf431e7cc4c4ff27e71059acc9638caa11ab35\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-cp37-none-any.whl size=11732 sha256=69f45fc6b11a5ebf44208a9a381c9601ef01589729a2a10115e5e374b78fa83b\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/8d/b4/8bfce5beea9a3496cc15b24961876adb7b6e2912ff09164179\n",
            "  Building wheel for pynisher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynisher: filename=pynisher-0.6.4-cp37-none-any.whl size=7045 sha256=734f9b51263ca3f31bc0b0044f2121417da29b91f631bb54735642ff6f75f39c\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/07/6b/c0e6d547d91cd50a30207421c3c3a63d71f195255c66401209\n",
            "  Building wheel for smac (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smac: filename=smac-0.13.1-cp37-none-any.whl size=252180 sha256=d48cbf750cf3519a4284c0d3a2d52e0a404b7d28d2f8a3b136d9de2cbc97cbaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/a6/af/9ec3c1ff517759ad1aad6babcbcf047dd5078c8b08fa4e63cc\n",
            "  Building wheel for lazy-import (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lazy-import: filename=lazy_import-0.2.2-py2.py3-none-any.whl size=16486 sha256=3057609b39a1850adbc387745e0d3aedcf86f5135eafcb2233097e6d75c28ff7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/b0/b5/8c7e6810aee14bc4ed4a542ce56e744126263bf4f4825a9094\n",
            "Successfully built auto-sklearn liac-arff pynisher smac lazy-import\n",
            "\u001b[31mERROR: distributed 2021.5.0 has requirement dask==2021.05.0, but you'll have dask 2.12.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: threadpoolctl, scikit-learn, cloudpickle, distributed, liac-arff, ConfigSpace, pynisher, pyrfr, lazy-import, smac, auto-sklearn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "Successfully installed ConfigSpace-0.4.19 auto-sklearn-0.12.6 cloudpickle-1.6.0 distributed-2021.5.0 lazy-import-0.2.2 liac-arff-2.5.0 pynisher-0.6.4 pyrfr-0.8.2 scikit-learn-0.24.2 smac-0.13.1 threadpoolctl-2.1.0\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Requirement already satisfied: scikit-learn==0.24.2 in /usr/local/lib/python3.7/dist-packages (0.24.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.19.5)\n",
            "Collecting dask\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/a0/0905a1112dc3801304348ac0af0e641a2fbe12fe163ab5c3a43b2e88092d/dask-2021.5.0-py3-none-any.whl (960kB)\n",
            "\u001b[K     |████████████████████████████████| 962kB 8.2MB/s \n",
            "\u001b[?25hRequirement already up-to-date: distributed in /usr/local/lib/python3.7/dist-packages (2021.5.0)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask) (1.6.0)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading https://files.pythonhosted.org/packages/41/94/360258a68b55f47859d72b2d0b2b3cfe0ca4fbbcb81b78812bd00ae86b7c/partd-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask) (0.11.1)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from dask) (3.13)\n",
            "Collecting fsspec>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 51.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from distributed) (5.1.1)\n",
            "Requirement already satisfied, skipping upgrade: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed) (7.1.2)\n",
            "Collecting locket\n",
            "  Downloading https://files.pythonhosted.org/packages/50/b8/e789e45b9b9c2db75e9d9e6ceb022c8d1d7e49b2c085ce8c05600f90a96b/locket-0.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed) (1.0.1)\n",
            "Installing collected packages: locket, partd, fsspec, dask\n",
            "  Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "Successfully installed dask-2021.5.0 fsspec-2021.5.0 locket-0.2.1 partd-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTCRpjlS5200"
      },
      "source": [
        "import re\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import string\n",
        "\n",
        "# dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "### Pre-processing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from collections import Counter\n",
        "\n",
        "## Classifiers\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "##Evaluation\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSKy5bESStmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9150111-8ad8-4c61-da1e-53243cf16ed6"
      },
      "source": [
        "# SpaCy Packages\n",
        "import spacy \n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_md\")\n",
        "# Gensim packages\n",
        "from gensim.parsing import strip_tags, strip_numeric, strip_multiple_whitespaces, stem_text, strip_punctuation, remove_stopwords\n",
        "from gensim.parsing import preprocess_string"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKTzJZ0p6Ty9"
      },
      "source": [
        "# loading dataset\n",
        "news_group = fetch_20newsgroups(subset='train')\n",
        "\n",
        "news_group_data = news_group.data\n",
        "news_group_target_names = news_group.target_names\n",
        "news_group_target = news_group.target\n",
        "\n",
        "# Creating a dataframe from the loaded data\n",
        "news_df = pd.DataFrame({'desc': news_group_data, \n",
        "                        'class': news_group_target})"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvmkwWxx6a8A",
        "outputId": "d675ef01-d7fe-425c-88af-2d3182215cd7"
      },
      "source": [
        "news_df.info()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11314 entries, 0 to 11313\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   desc    11314 non-null  object\n",
            " 1   class   11314 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 176.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "6UDp9UTI6lPN",
        "outputId": "adfce1ef-b8fa-42ff-aa9d-f7b36a057a1c"
      },
      "source": [
        "#Class distribution Visualization\n",
        "x = \"class\"\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.suptitle(x, fontsize=12)\n",
        "news_df[x].reset_index().groupby(x).count().sort_values(by= \n",
        "       \"index\").plot(kind=\"barh\", legend=False, \n",
        "        ax=ax).grid(axis='x')\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpeNthERz1yf"
      },
      "source": [
        "# Clean header\n",
        "def clean_header(text):\n",
        "    text = re.sub(r'(From:\\s+[^\\n]+\\n)', '', text)\n",
        "    text = re.sub(r'(Subject:[^\\n]+\\n)', '', text)\n",
        "    text = re.sub(r'(([\\sA-Za-z0-9\\-]+)?[A|a]rchive-name:[^\\n]+\\n)', '', text)\n",
        "    text = re.sub(r'(Last-modified:[^\\n]+\\n)', '', text)\n",
        "    text = re.sub(r'(Version:[^\\n]+\\n)', '', text)\n",
        "    text = re.sub(r'(lines:[^\\n]+\\n)', '', text)\n",
        "    return text\n",
        "\n",
        "news_df['desc'] =  news_df['desc'].apply(clean_header)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JElrHaCbs1K",
        "outputId": "9d57cb45-2b05-4430-c728-514c392985c9"
      },
      "source": [
        "news_df['desc'].head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Nntp-Posting-Host: rac3.wam.umd.edu\\nOrganizat...\n",
              "1    Summary: Final call for SI clock reports\\nKeyw...\n",
              "2    Organization: Purdue University Engineering Co...\n",
              "3    Organization: Harris Computer Systems Division...\n",
              "4    Organization: Smithsonian Astrophysical Observ...\n",
              "Name: desc, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtU7iJPgIemT"
      },
      "source": [
        "# Custom filter method\n",
        "transform_to_lower = lambda s: s.lower() #upper to lower case\n",
        "\n",
        "remove_single_char = lambda s: re.sub(r'\\s+\\w{1}\\s+', '', s) #regex to remove single char\n",
        "remove_double_char = lambda s: re.sub(r'\\s+\\w{2}\\s+', '', s) #regex to remove double char\n",
        "strip_non_alphanum =lambda s: re.sub(r\"^[a-zA-Z0-9_]*$\", '',s)\n",
        "remove_url = lambda s: re.sub(r'(?:http|ftp|https)://(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '',s)\n",
        "remove_email = lambda s: re.sub('(?:[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])', '', s)\n",
        "\n",
        "# Filters to be executed in pipeline\n",
        "CLEAN_FILTERS = [\n",
        "#                 strip_tags\n",
        "#                ,strip_numeric\n",
        "#                ,strip_punctuation\n",
        "#                ,strip_multiple_whitespaces\n",
        "#                ,transform_to_lower\n",
        "                remove_stopwords\n",
        "#                ,remove_single_char\n",
        "#                ,remove_url\n",
        "#                ,remove_email\n",
        "#                ,strip_non_alphanum\n",
        "#                 ,remove_double_char\n",
        "#                ,stem_text\n",
        "] #stemming text\n",
        "\n",
        "# Method does the filtering of all the unrelevant text elements\n",
        "def cleaning_pipe(document):\n",
        "    # Invoking gensim.parsing.preprocess_string method with set of filters\n",
        "    processed_words = preprocess_string(document, CLEAN_FILTERS)\n",
        "    \n",
        "    return processed_words"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8oTuskAGbu2"
      },
      "source": [
        "train_df, test_df = train_test_split(news_df, test_size=0.1, random_state=42, stratify=news_df[\"class\"])\n",
        "train_data = train_df[\"desc\"]\n",
        "train_target = train_df[\"class\"]\n",
        "test_data = test_df[\"desc\"]\n",
        "test_target = test_df[\"class\"]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMrk8X2-GTvq"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "with open('/content/drive/MyDrive/ATiML_Proj/glove.6B.50d.txt', \"rb\") as lines:\n",
        "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
        "           for line in lines}\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glzHf7PS_zZe",
        "outputId": "18fe8a47-c622-41b1-93c1-d812699bcd1b"
      },
      "source": [
        "import gensim\n",
        "# let X be a list of tokenized texts (i.e. list of lists of tokens)\n",
        "model = gensim.models.Word2Vec(news_df['desc'].apply(cleaning_pipe), size=100, window=5, min_count=5, workers=2)\n",
        "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tx-WMkRHwpf"
      },
      "source": [
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        # if a text is empty we should return a vector of zeros\n",
        "        # with the same dimensionality as all the other vectors\n",
        "        self.dim = len(word2vec.items())\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuwhssNNFuqj"
      },
      "source": [
        "class TfidfEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        self.word2weight = None\n",
        "        self.dim = len(word2vec.items())\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
        "        tfidf.fit(X)\n",
        "        # if a word was never seen - it must be at least as infrequent\n",
        "        # as any of the known words - so the default idf is the max of \n",
        "        # known idf's\n",
        "        max_idf = max(tfidf.idf_)\n",
        "        self.word2weight = defaultdict(\n",
        "            lambda: max_idf,\n",
        "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
        "                         for w in words if w in self.word2vec] or\n",
        "                        [np.zeros(self.dim)], axis=0)\n",
        "                for words in X\n",
        "            ])\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nevY5HfjGpGC"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "etree_w2v = Pipeline([\n",
        "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
        "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
        "etree_w2v_tfidf = Pipeline([\n",
        "    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
        "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux56LpHQGv7d"
      },
      "source": [
        "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(preprocessor= cleaning_pipe,analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
        "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(preprocessor= cleaning_pipe,analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
        "mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(preprocessor= cleaning_pipe,analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
        "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(preprocessor= cleaning_pipe,analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
        "svc = Pipeline([(\"count_vectorizer\", CountVectorizer(preprocessor= cleaning_pipe, analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])\n",
        "svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(preprocessor= cleaning_pipe, analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdb35rdEVRQ6",
        "outputId": "452510d6-cd1d-4ef8-d142-17f42c72224b"
      },
      "source": [
        "%%bash\n",
        "wget http://www.cs.umb.edu/~smimarog/textmining/datasets/20ng-test-no-stop.txt\n",
        "wget http://www.cs.umb.edu/~smimarog/textmining/datasets/20ng-train-no-stop.txt"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-28 00:18:06--  http://www.cs.umb.edu/~smimarog/textmining/datasets/20ng-test-no-stop.txt\n",
            "Resolving www.cs.umb.edu (www.cs.umb.edu)... 158.121.106.224\n",
            "Connecting to www.cs.umb.edu (www.cs.umb.edu)|158.121.106.224|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.cs.umb.edu:443/~smimarog/textmining/datasets/20ng-test-no-stop.txt [following]\n",
            "--2021-05-28 00:18:06--  https://www.cs.umb.edu/~smimarog/textmining/datasets/20ng-test-no-stop.txt\n",
            "Connecting to www.cs.umb.edu (www.cs.umb.edu)|158.121.106.224|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-05-28 00:18:07 ERROR 404: Not Found.\n",
            "\n",
            "--2021-05-28 00:18:07--  http://www.cs.umb.edu/~smimarog/textmining/datasets/20ng-train-no-stop.txt\n",
            "Resolving www.cs.umb.edu (www.cs.umb.edu)... 158.121.106.224\n",
            "Connecting to www.cs.umb.edu (www.cs.umb.edu)|158.121.106.224|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.cs.umb.edu:443/~smimarog/textmining/datasets/20ng-train-no-stop.txt [following]\n",
            "--2021-05-28 00:18:07--  https://www.cs.umb.edu/~smimarog/textmining/datasets/20ng-train-no-stop.txt\n",
            "Connecting to www.cs.umb.edu (www.cs.umb.edu)|158.121.106.224|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-05-28 00:18:07 ERROR 404: Not Found.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAh5X2Gbc9BB",
        "outputId": "802de583-e047-4c19-fd6f-c810f2c4033f"
      },
      "source": [
        "cv = StratifiedShuffleSplit( test_size=0.2, random_state=42)\n",
        "##BenchMark\n",
        "\n",
        "all_models = [\n",
        "#    (\"mult_nb\", mult_nb),\n",
        "#    (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
        "#    (\"bern_nb\", bern_nb),\n",
        "#    (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
        "    #(\"svc\", svc),\n",
        "    #(\"svc_tfidf\", svc_tfidf),\n",
        "    (\"w2v\", etree_w2v),\n",
        "    (\"w2v_tfidf\", etree_w2v_tfidf),\n",
        "]\n",
        "scores = sorted([(name, cross_val_score(model, train_data, train_target, scoring=\"accuracy\", cv=5).mean()) for name, model in all_models])\n",
        "print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'accuracy')))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model        accuracy\n",
            "---------  ----------\n",
            "w2v            0.2262\n",
            "w2v_tfidf      0.2374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMvkp4WRVk6J"
      },
      "source": [
        "import autosklearn.classification\n",
        "\n",
        "automl = autosklearn.classification.AutoSklearnClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkPSDYjHU_Hs"
      },
      "source": [
        "#BOW pipeline\n",
        "bow_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\"tfidf\", TfidfVectorizer(preprocessor=cleaning_pipe,analyzer=lambda x: x)),\n",
        "        (\"classifier\", automl),\n",
        "    ]\n",
        ")\n",
        "bow_pipeline.fit(train_data, train_target)\n",
        "y_pred = bow_pipeline.predict(test_data)\n",
        "cr = classification_report(test_target, y_pred)\n",
        "print('Classification report:\\n\\n{}'.format(\n",
        "    classification_report(test_target, y_pred))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfd08nHiW8K_"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")  # this model will give you 300D\n",
        "\n",
        "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, nlp):\n",
        "        self.nlp = nlp\n",
        "        self.dim = 100\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = X.apply(cleaning_pipe)\n",
        "        return X\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Doc.vector defaults to an average of the token vectors.\n",
        "        # https://spacy.io/api/doc#vector\n",
        "        return [self.nlp(text).vector for text in X]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLh1frODTwNw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "b5809a95-93ec-40a6-b4cd-a0d672d9166d"
      },
      "source": [
        "from sklearn import metrics, svm\n",
        "\n",
        "ftree_glove = Pipeline(\n",
        "    steps=[\n",
        "        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
        "        (\"reduce_dim\", TruncatedSVD(50)),\n",
        "        (\"classifier\", automl),\n",
        "        \n",
        "    ])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9bd06de484ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"mean_embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpacyVectorTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"reduce_dim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"classifier\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     ])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'automl' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt0DYqE6UGiG"
      },
      "source": [
        "ftree_glove.fit(train_data, train_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYrg052QUIja"
      },
      "source": [
        "y_pred = ftree_glove.predict(test_data)\n",
        "print('Classification report:\\n\\n{}'.format(\n",
        "    classification_report(test_target, y_pred))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u26-aLffw6pA",
        "outputId": "d81e57de-128d-4191-f388-bb95b3e5ed71"
      },
      "source": [
        "#BOW pipeline\n",
        "bow_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\"tfidf\", TfidfVectorizer(preprocessor=cleaning_pipe,analyzer=lambda x: x)),\n",
        "        (\"classifier\", SVC(kernel=\"linear\")),\n",
        "    ]\n",
        ")\n",
        "bow_pipeline.fit(train_data, train_target)\n",
        "y_pred = bow_pipeline.predict(test_data)\n",
        "cr = classification_report(test_target, y_pred)\n",
        "print('Classification report:\\n\\n{}'.format(\n",
        "    classification_report(test_target, y_pred))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.35      0.34        48\n",
            "           1       0.13      0.16      0.14        58\n",
            "           2       0.22      0.12      0.15        59\n",
            "           3       0.23      0.19      0.21        59\n",
            "           4       0.39      0.16      0.22        58\n",
            "           5       0.44      0.25      0.32        59\n",
            "           6       0.35      0.47      0.40        59\n",
            "           7       0.14      0.19      0.16        59\n",
            "           8       0.28      0.35      0.31        60\n",
            "           9       0.29      0.27      0.28        60\n",
            "          10       0.19      0.08      0.11        60\n",
            "          11       0.19      0.62      0.29        60\n",
            "          12       0.23      0.10      0.14        59\n",
            "          13       0.32      0.22      0.26        59\n",
            "          14       0.11      0.05      0.07        59\n",
            "          15       0.26      0.72      0.38        60\n",
            "          16       0.21      0.13      0.16        55\n",
            "          17       0.32      0.41      0.36        56\n",
            "          18       1.00      0.04      0.08        47\n",
            "          19       0.00      0.00      0.00        38\n",
            "\n",
            "    accuracy                           0.25      1132\n",
            "   macro avg       0.28      0.24      0.22      1132\n",
            "weighted avg       0.28      0.25      0.22      1132\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atw3IjoUiYiI",
        "outputId": "6db4e849-1c3e-470c-df09-57272fcec98a"
      },
      "source": [
        "#BOW pipeline\n",
        "bow_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\"tfidf\", TfidfVectorizer(preprocessor=cleaning_pipe,analyzer=lambda x: x)),\n",
        "        (\"classifier\", RandomForestClassifier()),\n",
        "    ]\n",
        ")\n",
        "bow_pipeline.fit(train_data, train_target)\n",
        "y_pred = bow_pipeline.predict(test_data)\n",
        "cr = classification_report(test_target, y_pred)\n",
        "print('Classification report:\\n\\n{}'.format(\n",
        "    classification_report(test_target, y_pred))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.25      0.26        48\n",
            "           1       0.18      0.21      0.19        58\n",
            "           2       0.39      0.42      0.41        59\n",
            "           3       0.31      0.29      0.30        59\n",
            "           4       0.27      0.26      0.27        58\n",
            "           5       0.48      0.69      0.57        59\n",
            "           6       0.46      0.66      0.54        59\n",
            "           7       0.30      0.22      0.25        59\n",
            "           8       0.50      0.62      0.55        60\n",
            "           9       0.37      0.42      0.39        60\n",
            "          10       0.48      0.48      0.48        60\n",
            "          11       0.33      0.50      0.40        60\n",
            "          12       0.28      0.14      0.18        59\n",
            "          13       0.35      0.27      0.30        59\n",
            "          14       0.33      0.27      0.30        59\n",
            "          15       0.44      0.68      0.53        60\n",
            "          16       0.49      0.38      0.43        55\n",
            "          17       0.65      0.55      0.60        56\n",
            "          18       0.50      0.28      0.36        47\n",
            "          19       0.67      0.11      0.18        38\n",
            "\n",
            "    accuracy                           0.39      1132\n",
            "   macro avg       0.40      0.38      0.37      1132\n",
            "weighted avg       0.40      0.39      0.38      1132\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTstS68YYtRz",
        "outputId": "bf35c08b-9e9c-4ff7-ca7f-ffd08f7f0d21"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.44      0.45        48\n",
            "           1       0.34      0.36      0.35        58\n",
            "           2       0.51      0.51      0.51        59\n",
            "           3       0.47      0.46      0.46        59\n",
            "           4       0.46      0.43      0.45        58\n",
            "           5       0.62      0.58      0.60        59\n",
            "           6       0.54      0.64      0.58        59\n",
            "           7       0.47      0.32      0.38        59\n",
            "           8       0.64      0.65      0.64        60\n",
            "           9       0.46      0.52      0.48        60\n",
            "          10       0.51      0.58      0.55        60\n",
            "          11       0.63      0.63      0.63        60\n",
            "          12       0.45      0.49      0.47        59\n",
            "          13       0.41      0.56      0.47        59\n",
            "          14       0.51      0.44      0.47        59\n",
            "          15       0.57      0.67      0.62        60\n",
            "          16       0.56      0.53      0.54        55\n",
            "          17       0.77      0.73      0.75        56\n",
            "          18       0.60      0.55      0.58        47\n",
            "          19       0.50      0.24      0.32        38\n",
            "\n",
            "    accuracy                           0.52      1132\n",
            "   macro avg       0.52      0.52      0.52      1132\n",
            "weighted avg       0.52      0.52      0.52      1132\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa1newEZpeBc",
        "outputId": "92f8d35a-ef79-4590-d679-e54d5d5fad74"
      },
      "source": [
        "import sklearn.metrics\n",
        "y_hat = bow_pipeline.predict(test_data)\n",
        "\n",
        "print(\"Accuracy score\", sklearn.metrics.accuracy_score(test_target, y_hat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score 0.5220848056537103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v2-ZB8HDbX0",
        "outputId": "bb6948a3-672d-4da3-ee33-8e77b7a61280"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "# train word2vec on all the texts - both training and test set\n",
        "# we're not using test labels, just texts so this is fine\n",
        "model = Word2Vec(train_data, size=100, window=5, min_count=5, workers=2)\n",
        "model.wv.index2word\n",
        "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr4DZC3PEh4L"
      },
      "source": [
        "all_models = [\n",
        "    (\"ftree_glove\", ftree_glove),\n",
        "    \n",
        "]\n",
        "\n",
        "scores = sorted([(name, cross_val_score(model, train_data, train_target, scoring=\"accuracy\", cv=cv).mean())\n",
        "                 for name, model in all_models])\n",
        "print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'accuracy')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MytXcxyz4uB"
      },
      "source": [
        "import spacy \n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "nlp = spacy.load(\"en_core_web_md\")  # this model will gives you 300D\n",
        "\n",
        "cv = StratifiedShuffleSplit(n_splits=100, test_size=0.1, random_state=42)\n",
        "\n",
        "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, nlp):\n",
        "        self.nlp = nlp\n",
        "        self.dim = 300\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Doc.vector defaults to an average of the token vectors.\n",
        "        # https://spacy.io/api/doc#vector\n",
        "        return [self.nlp(text).vector for text in X]\n",
        "\n",
        "ftree_glove = Pipeline([(\"mean_embeddings\", SpacyVectorTransformer(nlp)),(\"reduce_dim\", TruncatedSVD(50)),(\"classifier\", RandomForestClassifier()),])\n",
        "\n",
        "all_models = [\n",
        "    (\"ftree_glove\", ftree_glove),\n",
        "    \n",
        "]\n",
        "\n",
        "scores = sorted([(name, cross_val_score(model, train_data, train_target, scoring=\"accuracy\", cv=cv).mean())\n",
        "                 for name, model in all_models])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd5_mBrBKgXF"
      },
      "source": [
        "print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'accuracy')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlFtQ9GyEbym"
      },
      "source": [
        "svc = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])\n",
        "svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEmE8Th3QkBY",
        "outputId": "0c679a74-54c3-4481-fef0-d52e7da5c2b9"
      },
      "source": [
        "!unzip /content/gdrive/MyDrive/ATiML_Proj/glove*.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open /content/gdrive/MyDrive/ATiML_Proj/glove*.zip, /content/gdrive/MyDrive/ATiML_Proj/glove*.zip.zip or /content/gdrive/MyDrive/ATiML_Proj/glove*.zip.ZIP.\n",
            "\n",
            "No zipfiles found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTQuQ_YyRJmk",
        "outputId": "3ed7b4a9-9e04-4330-9480-4c14b9966590"
      },
      "source": [
        "print('Indexing word vectors.')\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('/content/drive/MyDrive/ATiML_Proj/glove.6B.50d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZUNlh_JF1En",
        "outputId": "efbfef5e-fff5-411f-9c88-13ed4d628c9f"
      },
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.2)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbchHbrQFKLl"
      },
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.semi_supervised import LabelPropagation\n",
        "\n",
        "\n",
        "# Parameters\n",
        "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log')\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s_M7LunGeYP"
      },
      "source": [
        "import spacy \n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "nlp = spacy.load(\"en_core_web_md\")  # this model will gives you 300D\n",
        "\n",
        "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, nlp):\n",
        "        self.nlp = nlp\n",
        "        self.dim = 300\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Doc.vector defaults to an average of the token vectors.\n",
        "        # https://spacy.io/api/doc#vector\n",
        "        return [self.nlp(text).vector for text in X]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L76zfyW09fEu"
      },
      "source": [
        "# Supervised Pipeline\n",
        "pipeline = Pipeline([\n",
        "    (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
        "    (\"reduce_dim\", TruncatedSVD(50)),\n",
        "    ('clf', SGDClassifier(**sdg_params)),\n",
        "])\n",
        "# SelfTraining Pipeline\n",
        "st_pipeline = Pipeline([\n",
        "    (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
        "    (\"reduce_dim\", TruncatedSVD(50)),\n",
        "    ('clf', SelfTrainingClassifier(SGDClassifier(**sdg_params), verbose=True)),\n",
        "])\n",
        "# LabelSpreading Pipeline\n",
        "lp_pipeline = Pipeline([\n",
        "    (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
        "    (\"reduce_dim\", TruncatedSVD(50)),\n",
        "    # LabelSpreading does not support dense matrices\n",
        "    ('todense', FunctionTransformer(lambda x: x.todense())),\n",
        "    ('clf', LabelPropagation(max_iter=2000)),\n",
        "])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "id": "gl6Bz_KMFQ0g",
        "outputId": "750b5009-6edf-45a4-ac3f-0eb803630446"
      },
      "source": [
        "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "    eval_and_print_metrics(pipeline, train_data, train_target, test_data, test_target)\n",
        "\n",
        "    # select a mask of 20% of the train dataset\n",
        "    y_mask = np.random.rand(len(train_target)) < 0.2\n",
        "\n",
        "    # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "    X_20, y_20 = map(list, zip(*((x, y)\n",
        "                     for x, y, m in zip(train_data, train_target, y_mask) if m)))\n",
        "    print(\"Supervised SGDClassifier on 20% of the training data:\")\n",
        "    eval_and_print_metrics(pipeline, X_20, y_20, test_data, test_target)\n",
        "\n",
        "    # set the non-masked subset to be unlabeled\n",
        "    train_target[~y_mask] = -1\n",
        "    print(\"SelfTrainingClassifier on 20% of the training data (rest \"\n",
        "          \"is unlabeled):\")\n",
        "    eval_and_print_metrics(st_pipeline, train_data, train_target, test_data, test_target)\n",
        "\n",
        "    print(\"LabelSpreading on 20% of the data (rest is unlabeled):\")\n",
        "    eval_and_print_metrics(lp_pipeline, train_data, train_target, test_data, test_target)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Supervised SGDClassifier on 100% of the data:\n",
            "Number of training samples: 10182\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.595\n",
            "----------\n",
            "\n",
            "Supervised SGDClassifier on 20% of the training data:\n",
            "Number of training samples: 2040\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.539\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "Number of training samples: 10182\n",
            "Unlabeled samples in training set: 8142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/series.py:1021: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._where(~key, value, inplace=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "End of iteration 1, added 2563 new labels.\n",
            "End of iteration 2, added 802 new labels.\n",
            "End of iteration 3, added 352 new labels.\n",
            "End of iteration 4, added 406 new labels.\n",
            "End of iteration 5, added 183 new labels.\n",
            "End of iteration 6, added 244 new labels.\n",
            "End of iteration 7, added 376 new labels.\n",
            "End of iteration 8, added 155 new labels.\n",
            "End of iteration 9, added 42 new labels.\n",
            "End of iteration 10, added 125 new labels.\n",
            "Micro-averaged F1 score on test set: 0.515\n",
            "----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-786495e5a76f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0meval_and_print_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m'CI'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LabelSpreading on 20% of the data (rest is unlabeled):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0meval_and_print_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlp_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVg465DMOHm5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}