{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.linalg import get_blas_funcs\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "\n",
    "class Semi_EM_MultinomialNB():\n",
    "    \"\"\"\n",
    "    Naive Bayes classifier for multinomial models for semi-supervised learning.\n",
    "    \n",
    "    Use both labeled and unlabeled data to train NB classifier, update parameters\n",
    "    using unlabeled data, and all data to evaluate performance of classifier. Optimize\n",
    "    classifier using Expectation-Maximization algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None, max_iter=30, tol=1e-6, print_log_lkh=True):\n",
    "        self.alpha = alpha\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "        self.clf = MultinomialNB(alpha=self.alpha, fit_prior=self.fit_prior, class_prior=self.class_prior)\n",
    "        self.log_lkh = -np.inf # log likelihood\n",
    "        self.max_iter = max_iter # max number of EM iterations\n",
    "        self.tol = tol # tolerance of log likelihood increment\n",
    "        self.feature_log_prob_ = np.array([]) # Empirical log probability of features given a class, P(x_i|y).\n",
    "        self.coef_ = np.array([]) # Mirrors feature_log_prob_ for interpreting MultinomialNB as a linear model.\n",
    "        self.print_log_lkh = print_log_lkh # if True, print log likelihood during EM iterations\n",
    "\n",
    "    def fit(self, X_l, y_l, X_u):\n",
    "        \"\"\"\n",
    "        Initialize the parameter using labeled data only.\n",
    "        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.\n",
    "        \"\"\"\n",
    "        n_ul_docs = X_u.shape[0] # number of unlabeled samples\n",
    "        n_l_docs = X_l.shape[0] # number of labeled samples\n",
    "        # initialization (n_docs = n_ul_docs)\n",
    "        clf = deepcopy(self.clf)# build new copy of classifier\n",
    "        clf.fit(X_l, y_l) # use labeled data only to initialize classifier parameters\n",
    "        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration\n",
    "        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]\n",
    "        lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.T]) # log CP of doc given class [n_classes, n_docs]\n",
    "        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T) \n",
    "        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]\n",
    "        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]\n",
    "        expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "        self.clf = deepcopy(clf)\n",
    "        self.log_lkh = expectation\n",
    "        if self.print_log_lkh:\n",
    "            print(\"Initial expected log likelihood = %0.3f\\n\" % expectation)\n",
    "        # Loop until log likelihood does not improve\n",
    "        iter_count = 0 # count EM iteration\n",
    "        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):\n",
    "        # while (iter_count<self.max_iter):\n",
    "            iter_count += 1\n",
    "            if self.print_log_lkh:\n",
    "                print(\"EM iteration #%d\" % iter_count) # debug\n",
    "            # E-step: Estimate class membership of unlabeled documents\n",
    "            y_u = clf.predict(X_u)\n",
    "            # M-step: Re-estimate classifier parameters\n",
    "            X = vstack([X_l, X_u])\n",
    "            y = np.concatenate((y_l, y_u), axis=0)\n",
    "            clf.fit(X, y)\n",
    "            # check convergence: update log likelihood\n",
    "            p_c_d = clf.predict_proba(X_u)\n",
    "            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "            b_w_d = (X_u > 0) # words in each document\n",
    "            lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.transpose()]) # log CP of doc given class [n_classes, n_docs]\n",
    "            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose()) \n",
    "            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]\n",
    "            expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "            if self.print_log_lkh:\n",
    "                print(\"\\tExpected log likelihood = %0.3f\" % expectation)\n",
    "            if (expectation-self.log_lkh >= self.tol):\n",
    "                prev_log_lkh = self.log_lkh\n",
    "                self.log_lkh = expectation\n",
    "                self.clf = deepcopy(clf)\n",
    "            else:\n",
    "                break\n",
    "        self.feature_log_prob_ = self.clf.feature_log_prob_\n",
    "        self.coef_ = self.clf.coef_\n",
    "        return self\n",
    "\n",
    "    def fit_with_clustering(self, X_l, y_l, X_u, y_u=None):\n",
    "        \"\"\"\n",
    "        Initialize the parameter using both labeled and unlabeled data.\n",
    "        The classes of unlabeled data are assigned using similarity with labeled data.\n",
    "        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.\n",
    "        The label propagation can only use dense matrix, so it is quite time consuming.\n",
    "        \"\"\"\n",
    "        n_ul_docs = X_u.shape[0] # number of unlabeled samples\n",
    "        n_l_docs = X_l.shape[0] # number of labeled samples\n",
    "        # initialization (n_docs = n_ul_docs): \n",
    "        # assign class to unlabeled data using similarity with labeled data if y_u is not given\n",
    "        if (y_u==None):\n",
    "            label_prop_model = LabelSpreading(kernel='rbf', max_iter=5, n_jobs=-1)\n",
    "            y_u = np.array([-1.0]*n_ul_docs)\n",
    "            X = vstack([X_l, X_u])\n",
    "            y = np.concatenate((y_l, y_u), axis=0)\n",
    "            label_prop_model.fit(X.toarray(), y)\n",
    "            y_u = label_prop_model.predict(X_u.toarray())\n",
    "        y = np.concatenate((y_l, y_u), axis=0)\n",
    "        clf = deepcopy(self.clf)# build new copy of classifier\n",
    "        clf.fit(X, y) # use labeled data only to initialize classifier parameters\n",
    "        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration\n",
    "        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]\n",
    "        lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.T.toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
    "        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T.toarray()) \n",
    "        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]\n",
    "        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]\n",
    "        expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "        self.clf = deepcopy(clf)\n",
    "        self.log_lkh = expectation\n",
    "        if self.print_log_lkh:\n",
    "            print(\"Initial expected log likelihood = %0.3f\\n\" % expectation)\n",
    "        # Loop until log likelihood does not improve\n",
    "        iter_count = 0 # count EM iteration\n",
    "        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):\n",
    "        # while (iter_count<self.max_iter):\n",
    "            iter_count += 1\n",
    "            if self.print_log_lkh:\n",
    "                print(\"EM iteration #%d\" % iter_count) # debug\n",
    "            # E-step: Estimate class membership of unlabeled documents\n",
    "            y_u = clf.predict(X_u)\n",
    "            # M-step: Re-estimate classifier parameters\n",
    "            X = vstack([X_l, X_u])\n",
    "            y = np.concatenate((y_l, y_u), axis=0)\n",
    "            clf.fit(X, y)\n",
    "            # check convergence: update log likelihood\n",
    "            p_c_d = clf.predict_proba(X_u)\n",
    "            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "            b_w_d = (X_u > 0) # words in each document\n",
    "            lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.transpose().toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
    "            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose().toarray()) \n",
    "            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]\n",
    "            expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "            if self.print_log_lkh:\n",
    "                print(\"\\tExpected log likelihood = %0.3f\" % expectation)\n",
    "            if (expectation-self.log_lkh >= self.tol):\n",
    "                prev_log_lkh = self.log_lkh\n",
    "                self.log_lkh = expectation\n",
    "                self.clf = deepcopy(clf)\n",
    "            else:\n",
    "                break\n",
    "        self.feature_log_prob_ = self.clf.feature_log_prob_\n",
    "        self.coef_ = self.clf.coef_\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self, X_l, y_l, X_u=np.array([])):\n",
    "        \"\"\"\n",
    "        Initialize the parameter using labeled data only.\n",
    "        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.\n",
    "        This function can only be used after fit()\n",
    "        \"\"\"\n",
    "        n_ul_docs = X_u.shape[0] # number of unlabeled samples\n",
    "        n_l_docs = X_l.shape[0] # number of labeled samples\n",
    "        # initialization (n_docs = n_ul_docs)\n",
    "        clf = deepcopy(self.clf)# build new copy of classifier\n",
    "        clf.partial_fit(X_l, y_l) # use labeled data only to initialize classifier parameters\n",
    "        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration\n",
    "        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]\n",
    "        lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.T.toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
    "        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T.toarray()) \n",
    "        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]\n",
    "        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]\n",
    "        expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "        self.clf = deepcopy(clf)\n",
    "        self.log_lkh = expectation\n",
    "        print(\"Initial expected log likelihood = %0.3f\\n\" % expectation)\n",
    "        # Loop until log likelihood does not improve\n",
    "        iter_count = 0 # count EM iteration\n",
    "        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):\n",
    "        # while (iter_count<self.max_iter):\n",
    "            iter_count += 1\n",
    "            print(\"EM iteration #%d\" % iter_count) # debug\n",
    "            # E-step: Estimate class membership of unlabeled documents\n",
    "            y_u = clf.predict(X_u)\n",
    "            # M-step: Re-estimate classifier parameters\n",
    "            X = vstack([X_l, X_u])\n",
    "            y = np.concatenate((y_l, y_u), axis=0)\n",
    "            clf.partial_fit(X, y)\n",
    "            # check convergence: update log likelihood\n",
    "            p_c_d = clf.predict_proba(X_u)\n",
    "            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "            b_w_d = (X_u > 0) # words in each document\n",
    "            lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.transpose()]) # log CP of doc given class [n_classes, n_docs]\n",
    "            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose()) \n",
    "            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]\n",
    "            expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "            print(\"\\tExpected log likelihood = %0.3f\" % expectation)\n",
    "            if (expectation-self.log_lkh >= self.tol):\n",
    "                prev_log_lkh = self.log_lkh\n",
    "                self.log_lkh = expectation\n",
    "                self.clf = deepcopy(clf)\n",
    "            else:\n",
    "                break\n",
    "        self.feature_log_prob_ = self.clf.feature_log_prob_\n",
    "        self.coef_ = self.clf.coef_\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.clf.score(X, y)\n",
    "\n",
    "    def get_params(deep=True):\n",
    "        return self.clf.get_params(deep)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.clf.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and libraries\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit\n",
    "from sklearn import metrics\n",
    "\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from wordcloud import WordCloud \n",
    "#from Semi_EM_NB import Semi_EM_MultinomialNB\n",
    "from os import path\n",
    "from PIL import Image\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(X):\n",
    "        # Convert to list\n",
    "        data = X\n",
    "\n",
    "        # Remove Emails\n",
    "        data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "        # Remove new line characters\n",
    "        data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "        # Remove distracting single quotes\n",
    "        data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "        \n",
    "        def sent_to_words(sentences):\n",
    "            for sentence in sentences:\n",
    "                yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "        data_words = list(sent_to_words(data))\n",
    "\n",
    "        def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "            \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "            texts_out = []\n",
    "            for sent in texts:\n",
    "                doc = nlp(\" \".join(sent)) \n",
    "                texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "            return texts_out\n",
    "\n",
    "        # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "        # Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "        data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "        \n",
    "        #print(data_lemmatized[:1])\n",
    "        return(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(clf, data_X, data_y, unlabeled,X_u, n_folds=5):\n",
    "    print('=' * 80)\n",
    "    print(\"Validation: \")\n",
    "    print(clf)\n",
    "    kf = StratifiedKFold(n_splits=n_folds)\n",
    "    start_time = time()\n",
    "    train_scores = list() # training accuracy\n",
    "    fold_count = 1\n",
    "    original_clf = deepcopy(clf)\n",
    "    avg_accuracy = 0\n",
    "    for train_ids, valid_ids in kf.split(data_X, data_y):\n",
    "        cv_clf = deepcopy(original_clf)\n",
    "        print(\"Fold # %d\" % fold_count)\n",
    "        fold_count += 1\n",
    "        train_X, train_y, valid_X, valid_y = data_X[train_ids], data_y[train_ids], data_X[valid_ids], data_y[valid_ids]\n",
    "        if unlabeled==True:\n",
    "            cv_clf.fit(train_X, train_y, X_u)\n",
    "        else:\n",
    "            cv_clf.fit(train_X, train_y)\n",
    "        pred = cv_clf.predict(valid_X)\n",
    "        scores = dict()\n",
    "        scores['accuracy'] = metrics.accuracy_score(valid_y, pred)\n",
    "        scores['recall'] = metrics.recall_score(valid_y, pred, average='macro')\n",
    "        scores['precision'] = metrics.precision_score(valid_y, pred, average='macro')\n",
    "        scores['f1_score'] = metrics.f1_score(valid_y, pred, average='macro')\n",
    "        train_scores.append(scores)\n",
    "        avg_accuracy += scores['accuracy']\n",
    "    train_time = time() - start_time\n",
    "    print(\"Validation time: %0.3f seconds\" % train_time)\n",
    "    print(\"Average training accuracy: %0.3f\" % (avg_accuracy/n_folds))\n",
    "    return train_scores, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topK(classifier, vectorizer, categories, K=10):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    nrows, ncols = 5, 4\n",
    "    fig, axes = plt.subplots(figsize=(50, 40), nrows=nrows, ncols=ncols)\n",
    "    #d = path.dirname(__file__)\n",
    "    circle_mask = np.array(Image.open(path.join('./', \"circle.png\")))\n",
    "    for i, category in enumerate(categories):\n",
    "        topK = np.argsort(classifier.coef_[i])[-K:]\n",
    "        text = \" \".join(feature_names[topK])\n",
    "        print(\"%s: %s\" % (category, text))\n",
    "        wordcloud = WordCloud(background_color=\"white\", mask=circle_mask).generate(text)\n",
    "        axes[i//ncols, i%ncols].imshow(wordcloud, cmap=plt.cm.cool_r, interpolation='bilinear')\n",
    "        axes[i//ncols, i%ncols].axis(\"off\")\n",
    "        axes[i//ncols, i%ncols].set_title(category, fontweight=\"bold\", size=24)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:    15076\tTest set size:     3770\n"
     ]
    }
   ],
   "source": [
    "# Load data set with class labels and split into train and test set\n",
    "test_size_ratio = 0.2\n",
    "data_Xy = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), shuffle=True)\n",
    "category_names = data_Xy.target_names # text names of all categories\n",
    "train_X, test_X, train_y, test_y = train_test_split(data_Xy.data, data_Xy.target, test_size=test_size_ratio, stratify=data_Xy.target)\n",
    "print(\"Training set size: %8d\\tTest set size: %8d\" % (len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ok', 'ill', 'try', 'more', 'time', 'one', 'out', 'there', 'have', 'information', 'microscience', 'hard', 'drive', 'how', 'set', 'jumper', 'where', 'be', 'master', 'slave', 'configuration', 'gladly', 'accept', 'info', 'have', 'mer', 'sit', 'room', 'collect', 'dust', 'just', 'wait', 'install', 'huge', 'application', 'thank', 'advance', 'regulary', 'check', 'ide', 'harddisk', 'spec', 'be', 'post', 'here']\n"
     ]
    }
   ],
   "source": [
    "# preprocess train and test text data\n",
    "train_X_clean=remove_noise(train_X)\n",
    "print(train_X_clean[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_clean=remove_noise(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dictionary_LDA = corpora.Dictionary(train_X_clean)\n",
    "# Term Document Frequency\n",
    "train_corpus = [dictionary_LDA.doc2bow(data_lemmatized) for data_lemmatized in train_X_clean]\n",
    "# Build LDA model\n",
    "num_topics = 20\n",
    "lda_model = gensim.models.LdaModel(train_corpus, num_topics=num_topics, id2word=dictionary_LDA, passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                           eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, train_corpus, dictionary_LDA)\n",
    "vis\n",
    "train_topics = [lda_model[train_corpus[i]] for i in range(len(train_X_clean))]\n",
    "def topics_document_to_dataframe(topics_document, num_topics):\n",
    "        res = pd.DataFrame(columns=range(num_topics))\n",
    "        for topic_weight in topics_document:\n",
    "            res.loc[0, topic_weight[0]] = topic_weight[1]\n",
    "        return res\n",
    "train_features=pd.concat([topics_document_to_dataframe(topics_document, num_topics=num_topics) for topics_document in train_topics]) \\\n",
    "            .reset_index(drop=True).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Document Frequency\n",
    "test_corpus = [dictionary_LDA.doc2bow(data_lemmatized) for data_lemmatized in test_X_clean]\n",
    "test_topics = [lda_model[test_corpus[i]] for i in range(len(test_X_clean))]\n",
    "def topics_document_to_dataframe(topics_document, num_topics):\n",
    "        res = pd.DataFrame(columns=range(num_topics))\n",
    "        for topic_weight in topics_document:\n",
    "            res.loc[0, topic_weight[0]] = topic_weight[1]\n",
    "        return res\n",
    "test_features=pd.concat([topics_document_to_dataframe(topics_document, num_topics=num_topics) for topics_document in test_topics]) \\\n",
    "            .reset_index(drop=True).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5076, 20) (10000, 20)\n"
     ]
    }
   ],
   "source": [
    "# Divide train data set into labeled and unlabeled data sets\n",
    "split_ratio = 0.2 # labeled vs total(labeled+unlabeled)\n",
    "# X_l, X_u, y_l, y_u = train_test_split(train_vec, train_y, train_size=split_ratio, stratify=train_y)\n",
    "X_l, X_u, y_l, y_u = train_test_split(train_features, train_y, test_size=10000, stratify=train_y)\n",
    "experiments = np.logspace(2.3, 3.7, num=20, base=10, dtype='int')\n",
    "print (X_l.shape, X_u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_l=X_l.to_numpy()\n",
    "X_u=X_u.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_l),type(X_u),type(y_l),type(y_u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.031 seconds\n",
      "Average training accuracy: 0.392\n",
      "Number of labeled documents:    199\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.015 seconds\n",
      "Average training accuracy: 0.391\n",
      "Number of labeled documents:    236\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.015 seconds\n",
      "Average training accuracy: 0.414\n",
      "Number of labeled documents:    280\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.014 seconds\n",
      "Average training accuracy: 0.390\n",
      "Number of labeled documents:    331\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.015 seconds\n",
      "Average training accuracy: 0.407\n",
      "Number of labeled documents:    393\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.015 seconds\n",
      "Average training accuracy: 0.429\n",
      "Number of labeled documents:    466\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.014 seconds\n",
      "Average training accuracy: 0.424\n",
      "Number of labeled documents:    552\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.016 seconds\n",
      "Average training accuracy: 0.416\n",
      "Number of labeled documents:    654\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.015 seconds\n",
      "Average training accuracy: 0.414\n",
      "Number of labeled documents:    775\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.016 seconds\n",
      "Average training accuracy: 0.422\n",
      "Number of labeled documents:    918\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.016 seconds\n",
      "Average training accuracy: 0.432\n",
      "Number of labeled documents:   1088\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.022 seconds\n",
      "Average training accuracy: 0.435\n",
      "Number of labeled documents:   1289\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.023 seconds\n",
      "Average training accuracy: 0.438\n",
      "Number of labeled documents:   1528\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.021 seconds\n",
      "Average training accuracy: 0.450\n",
      "Number of labeled documents:   1810\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.021 seconds\n",
      "Average training accuracy: 0.442\n",
      "Number of labeled documents:   2145\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.022 seconds\n",
      "Average training accuracy: 0.444\n",
      "Number of labeled documents:   2542\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.023 seconds\n",
      "Average training accuracy: 0.447\n",
      "Number of labeled documents:   3012\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.025 seconds\n",
      "Average training accuracy: 0.440\n",
      "Number of labeled documents:   3569\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.027 seconds\n",
      "Average training accuracy: 0.447\n",
      "Number of labeled documents:   4229\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.033 seconds\n",
      "Average training accuracy: 0.443\n",
      "Number of labeled documents:   5011\n"
     ]
    }
   ],
   "source": [
    "# Cross validation for Naive Bayes classifier \n",
    "# using labeled data set only\n",
    "nb_cv_scores = list()\n",
    "nb_cv_times = list()\n",
    "for n_l_docs in experiments:\n",
    "    nb_clf = MultinomialNB(alpha=1e-2)\n",
    "    cv_scores, cv_time = cross_validation(nb_clf, X_l[:n_l_docs,], y_l[:n_l_docs],False,X_u)\n",
    "    nb_cv_scores.append(cv_scores)\n",
    "    nb_cv_times.append(cv_time)\n",
    "    print(\"Number of labeled documents: %6d\" % n_l_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 11.808 seconds\n",
      "Average training accuracy: 0.085\n",
      "Number of labeled documents:    199\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 11.961 seconds\n",
      "Average training accuracy: 0.080\n",
      "Number of labeled documents:    236\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 13.648 seconds\n",
      "Average training accuracy: 0.079\n",
      "Number of labeled documents:    280\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 16.031 seconds\n",
      "Average training accuracy: 0.066\n",
      "Number of labeled documents:    331\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 12.545 seconds\n",
      "Average training accuracy: 0.074\n",
      "Number of labeled documents:    393\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 14.323 seconds\n",
      "Average training accuracy: 0.073\n",
      "Number of labeled documents:    466\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 11.939 seconds\n",
      "Average training accuracy: 0.072\n",
      "Number of labeled documents:    552\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 12.152 seconds\n",
      "Average training accuracy: 0.073\n",
      "Number of labeled documents:    654\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 11.463 seconds\n",
      "Average training accuracy: 0.071\n",
      "Number of labeled documents:    775\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 10.743 seconds\n",
      "Average training accuracy: 0.137\n",
      "Number of labeled documents:    918\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 3.342 seconds\n",
      "Average training accuracy: 0.432\n",
      "Number of labeled documents:   1088\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 3.341 seconds\n",
      "Average training accuracy: 0.435\n",
      "Number of labeled documents:   1289\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 3.530 seconds\n",
      "Average training accuracy: 0.438\n",
      "Number of labeled documents:   1528\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 3.281 seconds\n",
      "Average training accuracy: 0.450\n",
      "Number of labeled documents:   1810\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 3.283 seconds\n",
      "Average training accuracy: 0.442\n",
      "Number of labeled documents:   2145\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 3.445 seconds\n",
      "Average training accuracy: 0.444\n",
      "Number of labeled documents:   2542\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 3.250 seconds\n",
      "Average training accuracy: 0.447\n",
      "Number of labeled documents:   3012\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 3.273 seconds\n",
      "Average training accuracy: 0.440\n",
      "Number of labeled documents:   3569\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 3.312 seconds\n",
      "Average training accuracy: 0.447\n",
      "Number of labeled documents:   4229\n",
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 3.290 seconds\n",
      "Average training accuracy: 0.443\n",
      "Number of labeled documents:   5011\n"
     ]
    }
   ],
   "source": [
    "# Cross validation for semisupervised EM Naive Bayes classifier \n",
    "# using both labeled and unlabeled data set\n",
    "em_nb_cv_scores = list()\n",
    "em_nb_cv_times = list()\n",
    "for n_l_docs in experiments:\n",
    "    em_nb_clf = Semi_EM_MultinomialNB(alpha=1e-2, tol=100, print_log_lkh=False) # semi supervised EM based Naive Bayes classifier\n",
    "    cv_scores, cv_time = cross_validation(em_nb_clf, X_l[:n_l_docs,], y_l[:n_l_docs],True,X_u)\n",
    "    em_nb_cv_scores.append(cv_scores)\n",
    "    em_nb_cv_times.append(cv_time)\n",
    "    print(\"Number of labeled documents: %6d\" % n_l_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Cross-Validation Average Accuracy vs Number of Labeled Documents in Training Set')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot CV accuracy comparisons\n",
    "nb_score_mean = list()\n",
    "nb_score_err = list()\n",
    "em_nb_score_mean = list()\n",
    "em_nb_score_err = list()\n",
    "for idx in range(len(experiments)):\n",
    "    nb_scores = [value['accuracy'] for value in nb_cv_scores[idx]]\n",
    "    nb_score_mean.append(np.mean(nb_scores))\n",
    "    nb_score_err.append(np.std(nb_scores))\n",
    "    em_nb_scores = [value['accuracy'] for value in em_nb_cv_scores[idx]]\n",
    "    em_nb_score_mean.append(np.mean(em_nb_scores))\n",
    "    em_nb_score_err.append(np.std(em_nb_scores))\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(experiments, nb_score_mean, color='b', linewidth=2, label='Traditional Naive Bayes')\n",
    "ax.errorbar(experiments, nb_score_mean, yerr=nb_score_err, fmt='s', color='b')\n",
    "ax.plot(experiments, em_nb_score_mean, color='r', linewidth=2, label='Semisupervised EM Naive Bayes')\n",
    "ax.errorbar(experiments, em_nb_score_mean, yerr=em_nb_score_err, fmt='o', color='r')\n",
    "ax.set_xlabel('Number of Labeled Documents')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlim(left=np.min(experiments)-10, right=np.max(experiments)+100)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks([200, 1000, 5000]) \n",
    "ax.set_xticklabels([r'$2\\times10^2$', r'$10^3$', r'$5\\times10^3$'], fontsize=15)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Cross-Validation Average Accuracy vs Number of Labeled Documents in Training Set')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
