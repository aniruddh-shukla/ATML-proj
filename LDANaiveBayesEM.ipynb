{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from copy import deepcopy\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.linalg import get_blas_funcs\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "# Import packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit\n",
    "from sklearn import metrics\n",
    "\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from wordcloud import WordCloud \n",
    "from os import path\n",
    "from PIL import Image\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Semi-supervised Multinomial NB with EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Semi_EM_MultinomialNB():\n",
    "    \"\"\"\n",
    "    Naive Bayes classifier for multinomial models for semi-supervised learning.\n",
    "    \n",
    "    Use both labeled and unlabeled data to train NB classifier, update parameters\n",
    "    using unlabeled data, and all data to evaluate performance of classifier. Optimize\n",
    "    classifier using Expectation-Maximization algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None, max_iter=30, tol=1e-6, print_log_lkh=True):\n",
    "        self.alpha = alpha\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "        self.clf = MultinomialNB(alpha=self.alpha, fit_prior=self.fit_prior, class_prior=self.class_prior)\n",
    "        self.log_lkh = -np.inf # log likelihood\n",
    "        self.max_iter = max_iter # max number of EM iterations\n",
    "        self.tol = tol # tolerance of log likelihood increment\n",
    "        self.feature_log_prob_ = np.array([]) # Empirical log probability of features given a class, P(x_i|y).\n",
    "        self.coef_ = np.array([]) # Mirrors feature_log_prob_ for interpreting MultinomialNB as a linear model.\n",
    "        self.print_log_lkh = print_log_lkh # if True, print log likelihood during EM iterations\n",
    "\n",
    "    def fit(self, X_l, y_l, X_u):\n",
    "        \"\"\"\n",
    "        Initialize the parameter using labeled data only.\n",
    "        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.\n",
    "        \"\"\"\n",
    "        n_ul_docs = X_u.shape[0] # number of unlabeled samples\n",
    "        n_l_docs = X_l.shape[0] # number of labeled samples\n",
    "        # initialization (n_docs = n_ul_docs)\n",
    "        clf = deepcopy(self.clf)# build new copy of classifier\n",
    "        clf.fit(X_l, y_l) # use labeled data only to initialize classifier parameters\n",
    "        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration\n",
    "        lp_w_c = clf.feature_log_prob_ # log CP of feature given class [n_classes, n_words]\n",
    "        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]\n",
    "        lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.T]) # log CP of doc given class [n_classes, n_docs]\n",
    "        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T) \n",
    "        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]\n",
    "        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]\n",
    "        expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "        self.clf = deepcopy(clf)\n",
    "        self.log_lkh = expectation\n",
    "        if self.print_log_lkh:\n",
    "            print(\"Initial expected log likelihood = %0.3f\\n\" % expectation)\n",
    "        # Loop until log likelihood does not improve\n",
    "        iter_count = 0 # count EM iteration\n",
    "        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):\n",
    "        # while (iter_count<self.max_iter):\n",
    "            iter_count += 1\n",
    "            if self.print_log_lkh:\n",
    "                print(\"EM iteration #%d\" % iter_count) # debug\n",
    "            # E-step: Estimate class membership of unlabeled documents\n",
    "            y_u = clf.predict(X_u)\n",
    "            # M-step: Re-estimate classifier parameters\n",
    "            X = vstack([X_l, X_u])\n",
    "            y = np.concatenate((y_l, y_u), axis=0)\n",
    "            clf.fit(X, y)\n",
    "            # check convergence: update log likelihood\n",
    "            p_c_d = clf.predict_proba(X_u)\n",
    "            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "            b_w_d = (X_u > 0) # words in each document\n",
    "            lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.transpose()]) # log CP of doc given class [n_classes, n_docs]\n",
    "            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose()) \n",
    "            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]\n",
    "            expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "            if self.print_log_lkh:\n",
    "                print(\"\\tExpected log likelihood = %0.3f\" % expectation)\n",
    "            if (expectation-self.log_lkh >= self.tol):\n",
    "                prev_log_lkh = self.log_lkh\n",
    "                self.log_lkh = expectation\n",
    "                self.clf = deepcopy(clf)\n",
    "            else:\n",
    "                break\n",
    "        self.feature_log_prob_ = self.clf.feature_log_prob_\n",
    "        self.coef_ = self.clf.coef_\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.clf.score(X, y)\n",
    "\n",
    "    def get_params(deep=True):\n",
    "        return self.clf.get_params(deep)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.clf.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(X):\n",
    "        # Convert to list\n",
    "        data = X\n",
    "\n",
    "        # Remove Emails\n",
    "        data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "        # Remove new line characters\n",
    "        data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "        # Remove single quotes\n",
    "        data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "        \n",
    "        def sent_to_words(sentences):\n",
    "            for sentence in sentences:\n",
    "                yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "        data_words = list(sent_to_words(data))\n",
    "        \n",
    "        def remove_stopwords(texts):\n",
    "            return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "        \n",
    "        # Remove Stop Words\n",
    "        data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "        def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "            texts_out = []\n",
    "            for sent in texts:\n",
    "                doc = nlp(\" \".join(sent)) \n",
    "                texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "            return texts_out\n",
    "\n",
    "        # Initialize spacy model\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "        # Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "        data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "        \n",
    "        return(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(clf, data_X, data_y, unlabeled,X_u,n_folds=5):\n",
    "    print('=' * 80)\n",
    "    print(\"Validation: \")\n",
    "    print(clf)\n",
    "    kf = StratifiedKFold(n_splits=n_folds)\n",
    "    start_time = time()\n",
    "    train_scores = list() # training accuracy\n",
    "    fold_count = 1\n",
    "    original_clf = deepcopy(clf)\n",
    "    avg_accuracy = 0\n",
    "    for train_ids, valid_ids in kf.split(data_X, data_y):\n",
    "        cv_clf = deepcopy(original_clf)\n",
    "        print(\"Fold # %d\" % fold_count)\n",
    "        fold_count += 1\n",
    "        train_X, train_y, valid_X, valid_y = data_X[train_ids], data_y[train_ids], data_X[valid_ids], data_y[valid_ids]\n",
    "        if unlabeled==True:\n",
    "            cv_clf.fit(train_X, train_y, X_u)\n",
    "        else:\n",
    "            cv_clf.fit(train_X, train_y)\n",
    "        pred = cv_clf.predict(valid_X)\n",
    "        scores = dict()\n",
    "        scores['accuracy'] = metrics.accuracy_score(valid_y, pred)\n",
    "        scores['recall'] = metrics.recall_score(valid_y, pred, average='macro')\n",
    "        scores['precision'] = metrics.precision_score(valid_y, pred, average='macro')\n",
    "        scores['f1_score'] = metrics.f1_score(valid_y, pred, average='macro')\n",
    "        train_scores.append(scores)\n",
    "        avg_accuracy += scores['f1_score']\n",
    "    train_time = time() - start_time\n",
    "    print(\"Validation time: %0.3f seconds\" % train_time)\n",
    "    print(\"Average training accuracy: %0.3f\" % (avg_accuracy/n_folds))\n",
    "    return train_scores, train_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset and splitting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set with class labels and split into train and test set\n",
    "test_size_ratio = 0.2\n",
    "data_Xy = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), shuffle=True)\n",
    "category_names = data_Xy.target_names # text names of all categories\n",
    "train_X, test_X, train_y, test_y = train_test_split(data_Xy.data, data_Xy.target, test_size=test_size_ratio, stratify=data_Xy.target)\n",
    "print(\"Training set size: %8d\\tTest set size: %8d\" % (len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess train and test text data\n",
    "train_X_clean=remove_noise(train_X)\n",
    "test_X_clean=remove_noise(test_X)\n",
    "x=[]\n",
    "train_X_clean_new=[]\n",
    "for row in train_X_clean:\n",
    "    x=[word for word in row if len(word)>2]\n",
    "    train_X_clean_new.append(x)\n",
    "print(train_X_clean_new[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_LDA = corpora.Dictionary(train_X_clean_new)\n",
    "\n",
    "# Term Document Frequency\n",
    "train_corpus = [dictionary_LDA.doc2bow(data_lemmatized) for data_lemmatized in train_X_clean_new]\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "            coherence_values = []\n",
    "            model_list = []\n",
    "            for num_topics in range(start, limit, step):\n",
    "                model = gensim.models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "                model_list.append(model)\n",
    "                coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "                coherence_values.append(coherencemodel.get_coherence())\n",
    "            return model_list, coherence_values\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary_LDA, corpus=train_corpus, texts=train_X_clean_new, start=2, limit=40, step=6)\n",
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating LDA topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "num_topics = 14\n",
    "lda_model = gensim.models.LdaModel(train_corpus, num_topics=num_topics, id2word=dictionary_LDA, passes=10, alpha=[0.01]*num_topics, eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "train_topics = [lda_model[train_corpus[i]] for i in range(len(train_X_clean_new))]\n",
    "def topics_document_to_dataframe(topics_document, num_topics):\n",
    "        res = pd.DataFrame(columns=range(num_topics))\n",
    "        for topic_weight in topics_document:\n",
    "            res.loc[0, topic_weight[0]] = topic_weight[1]\n",
    "        return res\n",
    "train_features=pd.concat([topics_document_to_dataframe(topics_document, num_topics=num_topics) for topics_document in train_topics]) \\\n",
    "            .reset_index(drop=True).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, train_corpus, dictionary_LDA)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Document Frequency\n",
    "test_corpus = [dictionary_LDA.doc2bow(data_lemmatized) for data_lemmatized in test_X_clean]\n",
    "test_topics = [lda_model[test_corpus[i]] for i in range(len(test_X_clean))]\n",
    "def topics_document_to_dataframe(topics_document, num_topics):\n",
    "        res = pd.DataFrame(columns=range(num_topics))\n",
    "        for topic_weight in topics_document:\n",
    "            res.loc[0, topic_weight[0]] = topic_weight[1]\n",
    "        return res\n",
    "test_features=pd.concat([topics_document_to_dataframe(topics_document, num_topics=num_topics) for topics_document in test_topics]) \\\n",
    "            .reset_index(drop=True).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting train set into labeled and unlabeled set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Divide train data set into labeled and unlabeled data sets\n",
    "X_l, X_u, y_l, y_u = train_test_split(train_features, train_y, test_size=10000, stratify=train_y)\n",
    "experiments = np.logspace(2.3, 3.7, num=20, base=10, dtype='int')\n",
    "X_l=X_l.to_numpy()\n",
    "X_u=X_u.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross validation for Naive Bayes classifier \n",
    "# using labeled data set only\n",
    "nb_cv_scores = list()\n",
    "nb_cv_times = list()\n",
    "nb_cv_test_scores = list()\n",
    "for n_l_docs in experiments:\n",
    "    nb_clf = MultinomialNB(alpha=1e-2)\n",
    "    cv_scores, cv_time = cross_validation(nb_clf, X_l[:n_l_docs,], y_l[:n_l_docs],False,X_u)\n",
    "    nb_cv_scores.append(cv_scores)\n",
    "    nb_cv_times.append(cv_time)\n",
    "    print(\"Number of labeled documents: %6d\" % n_l_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation using Semi-supervised NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation for semisupervised EM Naive Bayes classifier \n",
    "# using both labeled and unlabeled data set\n",
    "em_nb_cv_scores = list()\n",
    "em_nb_cv_times = list()\n",
    "em_nb_cv_test_scores = list()\n",
    "for n_l_docs in experiments:\n",
    "    em_nb_clf = Semi_EM_MultinomialNB(alpha=1e-2, tol=100, print_log_lkh=False) # semi supervised EM based Naive Bayes classifier\n",
    "    cv_scores, cv_time= cross_validation(em_nb_clf, X_l[:n_l_docs,], y_l[:n_l_docs],True,X_u)\n",
    "    em_nb_cv_scores.append(cv_scores)\n",
    "    em_nb_cv_times.append(cv_time)\n",
    "    print(\"Number of labeled documents: %6d\" % n_l_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot CV accuracy comparisons\n",
    "nb_score_mean = list()\n",
    "nb_score_err = list()\n",
    "em_nb_score_mean = list()\n",
    "em_nb_score_err = list()\n",
    "nb_score_test_err = list()\n",
    "for idx in range(len(experiments)):\n",
    "    nb_scores = [value['accuracy'] for value in nb_cv_scores[idx]]\n",
    "    nb_score_mean.append(np.mean(nb_scores))\n",
    "    nb_score_err.append(np.std(nb_scores))\n",
    "    \n",
    "    em_nb_scores = [value['accuracy'] for value in em_nb_cv_scores[idx]]\n",
    "    em_nb_score_mean.append(np.mean(em_nb_scores))\n",
    "    em_nb_score_err.append(np.std(em_nb_scores))\n",
    "    \n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(experiments, nb_score_mean, color='b', linewidth=2, label='Traditional Naive Bayes')\n",
    "ax.errorbar(experiments, nb_score_mean, yerr=nb_score_err, fmt='s', color='b')\n",
    "\n",
    "ax.plot(experiments, em_nb_score_mean, color='r', linewidth=2, label='Semisupervised EM Naive Bayes')\n",
    "ax.errorbar(experiments, em_nb_score_mean, yerr=em_nb_score_err, fmt='o', color='r')\n",
    "\n",
    "ax.set_xlabel('Number of Labeled Documents')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlim(left=np.min(experiments)-10, right=np.max(experiments)+100)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks([200, 1000, 5000]) \n",
    "ax.set_xticklabels([r'$2\\times10^2$', r'$10^3$', r'$5\\times10^3$'], fontsize=15)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Cross-Validation Average Accuracy vs Number of Labeled Documents Training Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot CV F1 Score comparisons\n",
    "nb_score_mean = list()\n",
    "nb_score_err = list()\n",
    "em_nb_score_mean = list()\n",
    "em_nb_score_err = list()\n",
    "for idx in range(len(experiments)):\n",
    "    nb_scores = [value['f1_score'] for value in nb_cv_scores[idx]]\n",
    "    nb_score_mean.append(np.mean(nb_scores))\n",
    "    nb_score_err.append(np.std(nb_scores))\n",
    "    em_nb_scores = [value['f1_score'] for value in em_nb_cv_scores[idx]]\n",
    "    em_nb_score_mean.append(np.mean(em_nb_scores))\n",
    "    em_nb_score_err.append(np.std(em_nb_scores))\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(experiments, nb_score_mean, color='b', linestyle='--', linewidth=2, label='Traditional Naive Bayes')\n",
    "ax.errorbar(experiments, nb_score_mean, yerr=nb_score_err, fmt='s', color='b')\n",
    "ax.plot(experiments, em_nb_score_mean, color='r', linestyle='--', linewidth=2, label='Semisupervised EM Naive Bayes')\n",
    "ax.errorbar(experiments, em_nb_score_mean, yerr=em_nb_score_err, fmt='o', color='r')\n",
    "ax.fill_between(experiments, nb_score_mean, em_nb_score_mean, color='lightgreen')\n",
    "ax.set_xlabel('Number of Labeled Documents')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_xlim(left=np.min(experiments)-10, right=np.max(experiments)+100)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks([200, 1000, 5000]) \n",
    "ax.set_xticklabels([r'$2\\times10^2$', r'$10^3$', r'$5\\times10^3$'], fontsize=15)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Cross-Validation Average F1 Score vs Number of Labeled Documents in Training Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot CV accuracy comparisons\n",
    "nb_score_mean = list()\n",
    "nb_score_err = list()\n",
    "em_nb_score_mean = list()\n",
    "em_nb_score_err = list()\n",
    "for idx in range(len(experiments)):\n",
    "    nb_scores = [value['recall'] for value in nb_cv_scores[idx]]\n",
    "    nb_score_mean.append(np.mean(nb_scores))\n",
    "    nb_score_err.append(np.std(nb_scores))\n",
    "    em_nb_scores = [value['recall'] for value in em_nb_cv_scores[idx]]\n",
    "    em_nb_score_mean.append(np.mean(em_nb_scores))\n",
    "    em_nb_score_err.append(np.std(em_nb_scores))\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(experiments, nb_score_mean, color='b', linestyle='-.', linewidth=2, label='Traditional Naive Bayes')\n",
    "ax1.errorbar(experiments, nb_score_mean, yerr=nb_score_err, fmt='s', color='b')\n",
    "ax1.plot(experiments, em_nb_score_mean, color='r', linestyle='-.', linewidth=2, label='Semisupervised EM Naive Bayes')\n",
    "ax1.errorbar(experiments, em_nb_score_mean, yerr=em_nb_score_err, fmt='o', color='r')\n",
    "ax1.fill_between(experiments, nb_score_mean, em_nb_score_mean, color='pink')\n",
    "ax1.set_xlabel('Number of Labeled Documents')\n",
    "ax1.set_ylabel('Recall')\n",
    "ax1.set_xlim(left=np.min(experiments)-10, right=np.max(experiments)+100)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xticks([200, 1000, 5000]) \n",
    "ax1.set_xticklabels([r'$2\\times10^2$', r'$10^3$', r'$5\\times10^3$'], fontsize=15)\n",
    "nb_score_mean = list()\n",
    "nb_score_err = list()\n",
    "em_nb_score_mean = list()\n",
    "em_nb_score_err = list()\n",
    "for idx in range(len(experiments)):\n",
    "    nb_scores = [value['precision'] for value in nb_cv_scores[idx]]\n",
    "    nb_score_mean.append(np.mean(nb_scores))\n",
    "    nb_score_err.append(np.std(nb_scores))\n",
    "    em_nb_scores = [value['precision'] for value in em_nb_cv_scores[idx]]\n",
    "    em_nb_score_mean.append(np.mean(em_nb_scores))\n",
    "    em_nb_score_err.append(np.std(em_nb_scores))\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(experiments, nb_score_mean, color='b', linestyle=':', linewidth=2, label='Traditional Naive Bayes')\n",
    "ax2.errorbar(experiments, nb_score_mean, yerr=nb_score_err, fmt='s', color='b')\n",
    "ax2.plot(experiments, em_nb_score_mean, color='r', linestyle=':', linewidth=2, label='Semisupervised EM Naive Bayes')\n",
    "ax2.errorbar(experiments, em_nb_score_mean, yerr=em_nb_score_err, fmt='o', color='r')\n",
    "ax2.fill_between(experiments, nb_score_mean, em_nb_score_mean, color='aqua')\n",
    "ax2.set_xlabel('Number of Labeled Documents')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_xlim(left=np.min(experiments)-10, right=np.max(experiments)+100)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xticks([200, 1000, 5000]) \n",
    "ax2.set_xticklabels([r'$2\\times10^2$', r'$10^3$', r'$5\\times10^3$'], fontsize=15)\n",
    "fig.suptitle('Cross-Validation Recall and Precision vs Number of Labeled Documents in Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate original NB classifier using test data set\n",
    "nb_test_scores = dict()\n",
    "nb_test_scores['accuracy'] = list()\n",
    "nb_test_scores['recall'] = list()\n",
    "nb_test_scores['precision'] = list()\n",
    "nb_test_scores['f1_score'] = list()\n",
    "for n_l_docs in experiments:\n",
    "    nb_clf = MultinomialNB(alpha=1e-2).fit(X_l[:n_l_docs,], y_l[:n_l_docs])\n",
    "    pred = nb_clf.predict(test_features)\n",
    "    accuracy = metrics.accuracy_score(test_y, pred)\n",
    "    nb_test_scores['accuracy'].append(accuracy)\n",
    "    nb_test_scores['recall'].append(metrics.recall_score(test_y, pred, average='macro'))\n",
    "    nb_test_scores['precision'].append(metrics.precision_score(test_y, pred, average='macro'))\n",
    "    nb_test_scores['f1_score'].append(metrics.f1_score(test_y, pred, average='macro'))\n",
    "    print(\"%6d labeled documents lead to accuracy of %.3f\" % (n_l_docs, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate semi-supervised EM NB classifier using test data set\n",
    "em_nb_test_scores = dict()\n",
    "em_nb_test_scores['accuracy'] = list()\n",
    "em_nb_test_scores['recall'] = list()\n",
    "em_nb_test_scores['precision'] = list()\n",
    "em_nb_test_scores['f1_score'] = list()\n",
    "for n_l_docs in experiments:\n",
    "    em_nb_clf = Semi_EM_MultinomialNB(alpha=1e-2, tol=100, print_log_lkh=False).fit(X_l[:n_l_docs,], y_l[:n_l_docs], X_u)\n",
    "    pred = em_nb_clf.predict(test_features)\n",
    "    accuracy = metrics.accuracy_score(test_y, pred)\n",
    "    em_nb_test_scores['accuracy'].append(accuracy)\n",
    "    em_nb_test_scores['recall'].append(metrics.recall_score(test_y, pred, average='macro'))\n",
    "    em_nb_test_scores['precision'].append(metrics.precision_score(test_y, pred, average='macro'))\n",
    "    em_nb_test_scores['f1_score'].append(metrics.f1_score(test_y, pred, average='macro'))\n",
    "    print(\"%6d labeled documents lead to accuracy of %.3f\" % (n_l_docs, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and compare test accuracy metrics\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(experiments, nb_test_scores['accuracy'], color='b', marker='s', linewidth=2, label='Traditional Naive Bayes')\n",
    "ax.plot(experiments, em_nb_test_scores['accuracy'], color='r', marker='o', linewidth=2, label='Semisupervised EM Naive Bayes')\n",
    "ax.set_xlabel('Number of Labeled Documents')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlim(left=np.min(experiments)-10, right=np.max(experiments)+100)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks([200, 1000, 5000]) \n",
    "ax.set_xticklabels([r'$2\\times10^2$', r'$10^3$', r'$5\\times10^3$'], fontsize=15)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Test Accuracy vs Number of Labeled Documents in Training Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and compare test f1 score metrics\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(experiments, nb_test_scores['f1_score'], color='b', marker='s', linestyle='--', linewidth=2, label='Traditional Naive Bayes')\n",
    "ax.plot(experiments, em_nb_test_scores['f1_score'], color='r', marker='o', linestyle='--', linewidth=2, label='Semisupervised EM Naive Bayes')\n",
    "ax.fill_between(experiments, nb_test_scores['f1_score'], em_nb_test_scores['f1_score'], color='lightgreen')\n",
    "ax.set_xlabel('Number of Labeled Documents')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_xlim(left=np.min(experiments)-10, right=np.max(experiments)+100)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks([200, 1000, 5000]) \n",
    "ax.set_xticklabels([r'$2\\times10^2$', r'$10^3$', r'$5\\times10^3$'], fontsize=15)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Test F1 Score vs Number of Labeled Documents in Training Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and compare test recall and precisions metrics\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(experiments, nb_test_scores['recall'], color='b', marker='s', linestyle='-.', linewidth=2, label='Traditional Naive Bayes')\n",
    "ax1.plot(experiments, em_nb_test_scores['recall'], color='r', marker='o', linestyle='-.', linewidth=2, label='Semisupervised EM Naive Bayes')\n",
    "ax1.fill_between(experiments, nb_test_scores['recall'], em_nb_test_scores['recall'], color='pink')\n",
    "ax1.set_xlabel('Number of Labeled Documents')\n",
    "ax1.set_ylabel('Recall')\n",
    "ax1.set_xlim(left=np.min(experiments)-10, right=np.max(experiments)+100)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xticks([200, 1000, 5000]) \n",
    "ax1.set_xticklabels([r'$2\\times10^2$', r'$10^3$', r'$5\\times10^3$'], fontsize=15)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(experiments, nb_test_scores['precision'], color='b', marker='s', linestyle=':', linewidth=2, label='Traditional Naive Bayes')\n",
    "ax2.plot(experiments, em_nb_test_scores['precision'], color='r', marker='o', linestyle=':', linewidth=2, label='Semisupervised EM Naive Bayes')\n",
    "ax2.fill_between(experiments, nb_test_scores['precision'], em_nb_test_scores['precision'], color='aqua')\n",
    "ax2.set_xlabel('Number of Labeled Documents')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_xlim(left=np.min(experiments)-10, right=np.max(experiments)+100)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xticks([200, 1000, 5000]) \n",
    "ax2.set_xticklabels([r'$2\\times10^2$', r'$10^3$', r'$5\\times10^3$'], fontsize=15)\n",
    "fig.suptitle('Test Recall and Precision vs Number of Labeled Documents in Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details of classification accuracy metrics\n",
    "print(metrics.classification_report(test_y, pred, target_names=category_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix for test dataset\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.set()\n",
    "sns.heatmap(metrics.confusion_matrix(test_y, pred), annot=True, fmt=\"d\", linewidths=.5)\n",
    "plt.title(\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LDA with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(train_corpus)\n",
    "corpus_tfidf = tfidf[train_corpus]\n",
    "# Build LDA model\n",
    "num_topics = 14\n",
    "lda_model_tfidf = gensim.models.LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary_LDA, passes=10, alpha=[0.01]*num_topics, eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "train_topics = [lda_model_tfidf[corpus_tfidf[i]] for i in range(len(train_X_clean_new))]\n",
    "def topics_document_to_dataframe(topics_document, num_topics):\n",
    "        res = pd.DataFrame(columns=range(num_topics))\n",
    "        for topic_weight in topics_document:\n",
    "            res.loc[0, topic_weight[0]] = topic_weight[1]\n",
    "        return res\n",
    "train_features_tfidf=pd.concat([topics_document_to_dataframe(topics_document, num_topics=num_topics) for topics_document in train_topics]) \\\n",
    "            .reset_index(drop=True).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus_tfidf = tfidf[[dictionary_LDA.doc2bow(data_lemmatized) for data_lemmatized in test_X_clean]]\n",
    "test_topics = [lda_model_tfidf[test_corpus_tfidf[i]] for i in range(len(test_X_clean))]\n",
    "def topics_document_to_dataframe(topics_document, num_topics):\n",
    "        res = pd.DataFrame(columns=range(num_topics))\n",
    "        for topic_weight in topics_document:\n",
    "            res.loc[0, topic_weight[0]] = topic_weight[1]\n",
    "        return res\n",
    "test_features_tfidf=pd.concat([topics_document_to_dataframe(topics_document, num_topics=num_topics) for topics_document in test_topics]) \\\n",
    "            .reset_index(drop=True).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model_tfidf, corpus_tfidf, dictionary_LDA)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide train data set into labeled and unlabeled data sets\n",
    "X_l_tfidf, X_u_tfidf, y_l_tfidf, y_u_tfidf = train_test_split(train_features_tfidf, train_y, test_size=10000, stratify=train_y)\n",
    "experiments = np.logspace(2.3, 3.7, num=20, base=10, dtype='int')\n",
    "X_l_tfidf=X_l_tfidf.to_numpy()\n",
    "X_u_tfidf=X_u_tfidf.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation for Naive Bayes classifier \n",
    "# using labeled data set only\n",
    "nb_cv_scores_tfidf = list()\n",
    "nb_cv_times_tfidf = list()\n",
    "nb_cv_test_scores_tfidf = list()\n",
    "for n_l_docs in experiments:\n",
    "    nb_clf_tfidf = MultinomialNB(alpha=1e-2)\n",
    "    cv_scores_tfidf, cv_time_tfidf = cross_validation(nb_clf_tfidf, X_l_tfidf[:n_l_docs,], y_l_tfidf[:n_l_docs],False,X_u_tfidf)\n",
    "    nb_cv_scores_tfidf.append(cv_scores_tfidf)\n",
    "    nb_cv_times_tfidf.append(cv_time_tfidf)\n",
    "    print(\"Number of labeled documents: %6d\" % n_l_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation for semisupervised EM Naive Bayes classifier \n",
    "# using both labeled and unlabeled data set\n",
    "em_nb_cv_scores_tfidf = list()\n",
    "em_nb_cv_times_tfidf = list()\n",
    "em_nb_cv_test_scores_tfidf = list()\n",
    "for n_l_docs in experiments:\n",
    "    em_nb_clf_tfidf = Semi_EM_MultinomialNB(alpha=1e-2, tol=100, print_log_lkh=False) # semi supervised EM based Naive Bayes classifier\n",
    "    cv_scores_tfidf, cv_time_tfidf= cross_validation(em_nb_clf_tfidf, X_l_tfidf[:n_l_docs,], y_l_tfidf[:n_l_docs],True,X_u_tfidf)\n",
    "    em_nb_cv_scores_tfidf.append(cv_scores_tfidf)\n",
    "    em_nb_cv_times_tfidf.append(cv_time_tfidf)\n",
    "    print(\"Number of labeled documents: %6d\" % n_l_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
