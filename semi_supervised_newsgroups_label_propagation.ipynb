{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "semi-supervised_newsgroups_label_propagation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniruddh-shukla/ATML-proj/blob/main/semi_supervised_newsgroups_label_propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3zvDFz_BzJu",
        "outputId": "f7a8ad38-264c-4fda-ad27-13f35d4a6eba"
      },
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGPF15L1WUt9",
        "outputId": "282de263-5463-4135-cb06-b975f1ebed64"
      },
      "source": [
        "import re\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "# dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Gensim packages\n",
        "from gensim.parsing import strip_tags, strip_numeric, strip_multiple_whitespaces, stem_text, strip_punctuation, remove_stopwords\n",
        "from gensim.parsing import preprocess_string\n",
        "# loading dataset\n",
        "news_group = fetch_20newsgroups(subset='train')\n",
        "\n",
        "news_group_data = news_group.data\n",
        "news_group_target_names = news_group.target_names\n",
        "news_group_target = news_group.target\n",
        "# Creating a dataframe from the loaded data\n",
        "news_df = pd.DataFrame({'news': news_group_data, \n",
        "                        'class': news_group_target})\n",
        "# Custom filter method\n",
        "transform_to_lower = lambda s: s.lower() #upper to lower case\n",
        "\n",
        "remove_single_char = lambda s: re.sub(r'\\s+\\w{1}\\s+', '', s) #regex to remove single char\n",
        "\n",
        "# Filters to be executed in pipeline\n",
        "CLEAN_FILTERS = [strip_tags,\n",
        "                strip_numeric,\n",
        "                strip_punctuation, \n",
        "                strip_multiple_whitespaces, \n",
        "                transform_to_lower,\n",
        "                remove_stopwords,\n",
        "                remove_single_char,stem_text] #stemming text\n",
        "\n",
        "# Method does the filtering of all the unrelevant text elements\n",
        "def cleaning_pipe(document):\n",
        "    # Invoking gensim.parsing.preprocess_string method with set of filters\n",
        "    processed_words = preprocess_string(document, CLEAN_FILTERS)\n",
        "    \n",
        "    return processed_words"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rnrZ-6q-pzE",
        "outputId": "6f15de2a-a7db-4cbc-db47-98b5f091d3ee"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.semi_supervised import LabelPropagation\n",
        "data = fetch_20newsgroups(subset='train', categories=None)\n",
        "print(\"%d documents\" % len(data.filenames))\n",
        "print(\"%d categories\" % len(data.target_names))\n",
        "print()\n",
        "\n",
        "# Parameters\n",
        "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log')\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "# Supervised Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=cleaning_pipe,**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier(**sdg_params)),\n",
        "])\n",
        "# SelfTraining Pipeline\n",
        "st_pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=cleaning_pipe,**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SelfTrainingClassifier(SGDClassifier(**sdg_params), verbose=True)),\n",
        "])\n",
        "# LabelSpreading Pipeline\n",
        "lp_pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=cleaning_pipe,**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    # LabelSpreading does not support dense matrices\n",
        "    ('todense', FunctionTransformer(lambda x: x.todense())),\n",
        "    ('clf', LabelPropagation(max_iter=2000)),\n",
        "])\n",
        "\n",
        "\n",
        "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "    print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "    eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # select a mask of 20% of the train dataset\n",
        "    y_mask = np.random.rand(len(y_train)) < 0.2\n",
        "\n",
        "    # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "    X_20, y_20 = map(list, zip(*((x, y)\n",
        "                     for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "    print(\"Supervised SGDClassifier on 20% of the training data:\")\n",
        "    eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
        "\n",
        "    # set the non-masked subset to be unlabeled\n",
        "    y_train[~y_mask] = -1\n",
        "    print(\"SelfTrainingClassifier on 20% of the training data (rest \"\n",
        "          \"is unlabeled):\")\n",
        "    eval_and_print_metrics(st_pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    if 'CI' not in os.environ:\n",
        "        print(\"LabelSpreading on 20% of the data (rest is unlabeled):\")\n",
        "        eval_and_print_metrics(lp_pipeline, X_train, y_train, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11314 documents\n",
            "20 categories\n",
            "\n",
            "Supervised SGDClassifier on 100% of the data:\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.897\n",
            "----------\n",
            "\n",
            "Supervised SGDClassifier on 20% of the training data:\n",
            "Number of training samples: 1627\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.766\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 6858\n",
            "End of iteration 1, added 3317 new labels.\n",
            "End of iteration 2, added 697 new labels.\n",
            "End of iteration 3, added 257 new labels.\n",
            "End of iteration 4, added 89 new labels.\n",
            "End of iteration 5, added 39 new labels.\n",
            "End of iteration 6, added 32 new labels.\n",
            "End of iteration 7, added 19 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.817\n",
            "----------\n",
            "\n",
            "LabelSpreading on 20% of the data (rest is unlabeled):\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 6858\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}