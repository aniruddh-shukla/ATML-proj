{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oishi_ATIML_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "mbp72iRxX8Q7"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U textstat\n",
        "!pip install textblob\n",
        "!pip install --upgrade scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JIYuoyc-ZOPW",
        "outputId": "6506872d-a2f5-4c11-9eed-9e7dc7633f44"
      },
      "source": [
        "import re\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import textstat\n",
        "import string\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import preprocessing\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier, LabelSpreading\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6VpZbNtbU9u"
      },
      "source": [
        "# Fetch Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXxJPkRqZ4jt"
      },
      "source": [
        "news_group = fetch_20newsgroups(subset='train')\n",
        "news_group_data = news_group.data\n",
        "news_group_target_names = news_group.target_names\n",
        "news_group_target = news_group.target\n",
        "\n",
        "news_group_test = fetch_20newsgroups(subset='test')\n",
        "news_group_test_data = news_group_test.data\n",
        "news_group_test_target_names = news_group_test.target_names\n",
        "news_group_test_target = news_group_test.target"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb5VqcGucNSC"
      },
      "source": [
        "# Convert to Pandas DF and Random Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2G1qbWGcFzA"
      },
      "source": [
        "news_df = pd.DataFrame({'news': news_group_data, \n",
        "                        'class': news_group_target})\n",
        "\n",
        "news_sampled = news_df.sample(2000)\n",
        "news_sampled.reset_index(drop=True, inplace=True)\n",
        "\n",
        "news_df_test = pd.DataFrame({'news': news_group_test_data, \n",
        "                        'class': news_group_test_target})\n",
        "\n",
        "news_sampled_test = news_df_test.sample(400)\n",
        "news_sampled_test.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3btQC87c3Wa"
      },
      "source": [
        "# Cleaning Text\n",
        "\n",
        "*   Cleaning\n",
        "*   Removing stop words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI10L4oGDe-M"
      },
      "source": [
        "class Cleaner():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stop_words = stopwords.words('english')\n",
        "        self.re_url = re.compile(r'(?:http|ftp|https)://(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?')\n",
        "        self.re_email = re.compile('(?:[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])')\n",
        "\n",
        "    def clean_news(self, text):\n",
        "        text = re.sub(r'(From:\\s+[^\\n]+\\n)', '', text) # remove From\n",
        "        text = re.sub(r'(Subject:)', '', text) # remove the word \"Subject:\"\"\n",
        "        text = text.lower() # Convert to lowerCase\n",
        "        text = text.strip() # Strip terminal spaces\n",
        "        text = re.sub(self.re_url, '', text)\n",
        "        text = re.sub(self.re_email, '', text)       \n",
        "        text = re.sub(r'\\s+\\w{1}\\s+', ' ', text) #remove single char\n",
        "        #text = text.replace('\\n',' ')\n",
        "        text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) # punctuations\n",
        "        text = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', ' ', text) # remove pure digits\n",
        "        text = re.sub(r'(\\s+)', ' ', text) # replace >1 whitespaces with single space\n",
        "\n",
        "        return text\n",
        "\n",
        "    def removeStopWords(self, text):\n",
        "        \n",
        "        x = text.split(' ')\n",
        "        for word in x:\n",
        "            if(word in self.stop_words):\n",
        "                x = list(filter((word).__ne__, x))\n",
        "        return ' '.join(x)\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "    def transform(self, data):\n",
        "        cleaner = Cleaner()\n",
        "        \n",
        "        data_array = []\n",
        "        for d in data:\n",
        "            s = cleaner.clean_news(d)\n",
        "            w = cleaner.removeStopWords(s)\n",
        "            data_array.append(w)\n",
        "        return data_array "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0mlz46-Cqql"
      },
      "source": [
        "# BOW Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqHpexX0Ju8b"
      },
      "source": [
        "class BOWVectorizer():\n",
        "    def __init__(self):\n",
        "        self.vectorize = None\n",
        "    def fit(self, x, y=None):\n",
        "        bowvec = TfidfVectorizer()\n",
        "        bowvec.fit(x)\n",
        "        self.vectorize = bowvec\n",
        "        return self.vectorize\n",
        "    \n",
        "    def transform(self, data):\n",
        "        x = self.vectorize.transform(data)\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0sKlFk8QFlV"
      },
      "source": [
        "# POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-rv3AqYR0xp"
      },
      "source": [
        "class POSVectorizer():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def creatingPOSTags(self, x):\n",
        "             \n",
        "        pos_family = {'NOUN' : ['NN','NNS','NNP','NNPS'],\n",
        "                    'PRON' : ['PRP','PRP$','WP','WP$'], \n",
        "                    'VERB' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "                    'ADJ'  : ['JJ','JJR','JJS'],\n",
        "                    'ADV'  : ['RB','RBR','RBS','WRB']\n",
        "                    }\n",
        "            \n",
        "        count_pos = {'NOUN':0,'PRON':0,'VERB':0,'ADJ':0,'ADV':0}\n",
        "        \n",
        "        blob  = TextBlob(x) #converts sentences to tokens\n",
        "        for tuple in blob.tags: #blob tags contains term and its pos\n",
        "            #print(tuple)\n",
        "            pos = list(tuple)[1]\n",
        "            if pos in pos_family['NOUN']:\n",
        "                count_pos['NOUN'] = count_pos.get('NOUN')+1\n",
        "            elif pos in pos_family['PRON']:\n",
        "                count_pos['PRON'] = count_pos.get('PRON')+1\n",
        "            elif pos in pos_family['VERB']:\n",
        "                count_pos['VERB'] = count_pos.get('VERB')+1\n",
        "            elif pos in pos_family['ADJ']:\n",
        "                count_pos['ADJ'] = count_pos.get('ADJ')+1\n",
        "            elif pos in pos_family['ADV']:\n",
        "                count_pos['ADV'] = count_pos.get('ADV')+1\n",
        "        return count_pos \n",
        "    \n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "    def transform(self, data):\n",
        "        posVector = POSVectorizer()\n",
        "        pos_vect = []\n",
        "        for d in data:\n",
        "            pos_vect.append(posVector.creatingPOSTags(d))\n",
        "        return pos_vect"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwHFHrhoCwkV"
      },
      "source": [
        "# Convert toArray()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN4pqe_mEnmh"
      },
      "source": [
        "class ToArray():\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.toarray()\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):        \n",
        "        return self"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKc70oP3C2PG"
      },
      "source": [
        "# Creating Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L32vijlFEwMh"
      },
      "source": [
        "bow_transformer = Pipeline(\n",
        "    steps=[\n",
        "        (\"cleaner\", Cleaner()),\n",
        "        (\"bow\", BOWVectorizer()),\n",
        "        (\"toarray\", ToArray()), #converting toarray since minmax can't handle sparce matrix\n",
        "        (\"scale\", preprocessing.MinMaxScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "pos_transformer = Pipeline(\n",
        "    steps=[\n",
        "        (\"cleaner\", Cleaner()),\n",
        "        (\"pos\", POSVectorizer()),\n",
        "        (\"dict_vect\", DictVectorizer()),\n",
        "        (\"toarray\", ToArray()), #converting toarray since minmax can't handle sparce matrix\n",
        "        (\"scale\", preprocessing.MinMaxScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "combined_features = FeatureUnion(\n",
        "    transformer_list=[\n",
        "        (\"bow\", bow_transformer),\n",
        "        (\"pos\", pos_transformer),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def fitFinalPipeline(classifier, X_train, Y_train, X_test, Y_test):\n",
        "    final_pipeline = Pipeline(\n",
        "        steps=[\n",
        "            (\"combined_features\", combined_features),\n",
        "            ('chi',  SelectKBest(chi2, k=20000)),\n",
        "            (\"classifier\", classifier),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    final_pipeline.fit(X_train, Y_train)\n",
        "    y_pred = final_pipeline.predict(X_test)\n",
        "    cr = classification_report(Y_test, y_pred)\n",
        "    print(cr)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKIDhioaCaw3"
      },
      "source": [
        "# RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYIT8jWXE7Tu",
        "outputId": "5c018f5f-ad2d-445e-b15d-b925f4c3e613"
      },
      "source": [
        "fitFinalPipeline (RandomForestClassifier(), news_sampled['news'], news_sampled['class'], news_sampled_test['news'], news_sampled_test['class'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.78      0.72        18\n",
            "           1       0.76      0.76      0.76        17\n",
            "           2       0.49      0.86      0.62        22\n",
            "           3       0.61      0.58      0.59        19\n",
            "           4       0.52      0.62      0.57        21\n",
            "           5       0.89      0.64      0.74        25\n",
            "           6       0.81      0.68      0.74        19\n",
            "           7       0.75      0.86      0.80        21\n",
            "           8       0.81      0.81      0.81        21\n",
            "           9       0.59      0.72      0.65        18\n",
            "          10       0.88      1.00      0.94        22\n",
            "          11       0.75      0.90      0.82        10\n",
            "          12       0.88      0.30      0.45        23\n",
            "          13       0.50      0.65      0.57        20\n",
            "          14       0.96      0.80      0.87        30\n",
            "          15       0.70      0.91      0.79        23\n",
            "          16       0.70      0.89      0.78        18\n",
            "          17       1.00      0.79      0.88        24\n",
            "          18       0.80      0.42      0.55        19\n",
            "          19       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           0.71       400\n",
            "   macro avg       0.70      0.70      0.68       400\n",
            "weighted avg       0.73      0.71      0.70       400\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoJTgi8BQFlY"
      },
      "source": [
        "# SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM7qZgFxtAIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5864aeb-7c11-45b6-a615-3e6e0d882595"
      },
      "source": [
        "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log')\n",
        "\n",
        "fitFinalPipeline (SGDClassifier(**sdg_params), news_sampled['news'], news_sampled['class'], news_sampled_test['news'], news_sampled_test['class'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.44      0.57        18\n",
            "           1       0.65      0.65      0.65        17\n",
            "           2       0.46      0.82      0.59        22\n",
            "           3       0.56      0.26      0.36        19\n",
            "           4       0.47      0.67      0.55        21\n",
            "           5       0.94      0.60      0.73        25\n",
            "           6       0.71      0.63      0.67        19\n",
            "           7       0.83      0.71      0.77        21\n",
            "           8       0.76      0.76      0.76        21\n",
            "           9       0.63      0.67      0.65        18\n",
            "          10       0.84      0.73      0.78        22\n",
            "          11       0.86      0.60      0.71        10\n",
            "          12       0.61      0.48      0.54        23\n",
            "          13       0.75      0.45      0.56        20\n",
            "          14       0.90      0.90      0.90        30\n",
            "          15       0.72      0.57      0.63        23\n",
            "          16       0.37      0.83      0.51        18\n",
            "          17       0.54      0.58      0.56        24\n",
            "          18       0.35      0.32      0.33        19\n",
            "          19       0.06      0.10      0.08        10\n",
            "\n",
            "    accuracy                           0.61       400\n",
            "   macro avg       0.64      0.59      0.59       400\n",
            "weighted avg       0.66      0.61      0.62       400\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FN27lfqgezx"
      },
      "source": [
        "# Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqRvFeunaFkK",
        "outputId": "7f02e329-72cb-4dc1-a4dd-ad2fd43965bf"
      },
      "source": [
        "y_mask = np.random.rand(len(news_sampled['class'])) < 0.2\n",
        "y_masked_class = news_sampled\n",
        "y_masked_class['class'][~y_mask] = -1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdeh5mdFhEeb"
      },
      "source": [
        "# LabelSpreading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq3aPO-LhBz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909ff3ac-de09-40df-a8a2-3f97fa02d19d"
      },
      "source": [
        "fitFinalPipeline (LabelSpreading(gamma=0.25, max_iter=5), news_sampled['news'], y_masked_class['class'], news_sampled_test['news'], news_sampled_test['class'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/semi_supervised/_label_propagation.py:292: ConvergenceWarning: max_iter=5 was reached without convergence.\n",
            "  category=ConvergenceWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.00      0.00      0.00        17\n",
            "           2       0.06      1.00      0.11        22\n",
            "           3       1.00      0.05      0.10        19\n",
            "           4       0.00      0.00      0.00        21\n",
            "           5       1.00      0.04      0.08        25\n",
            "           6       0.00      0.00      0.00        19\n",
            "           7       0.00      0.00      0.00        21\n",
            "           8       0.00      0.00      0.00        21\n",
            "           9       0.00      0.00      0.00        18\n",
            "          10       0.00      0.00      0.00        22\n",
            "          11       0.00      0.00      0.00        10\n",
            "          12       0.00      0.00      0.00        23\n",
            "          13       0.00      0.00      0.00        20\n",
            "          14       0.00      0.00      0.00        30\n",
            "          15       0.00      0.00      0.00        23\n",
            "          16       0.00      0.00      0.00        18\n",
            "          17       0.00      0.00      0.00        24\n",
            "          18       0.00      0.00      0.00        19\n",
            "          19       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           0.06       400\n",
            "   macro avg       0.10      0.05      0.01       400\n",
            "weighted avg       0.11      0.06      0.02       400\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/semi_supervised/_label_propagation.py:205: RuntimeWarning: invalid value encountered in true_divide\n",
            "  probabilities /= normalizer\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pgFtYFWgi0F"
      },
      "source": [
        "# SelfTrainingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhJG2zvsXij3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675a6cd1-c454-4dcc-b03f-abad7b5adfcb"
      },
      "source": [
        "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log')\n",
        "fitFinalPipeline (SelfTrainingClassifier(SGDClassifier(**sdg_params), verbose=True), news_sampled['news'], y_masked_class['class'], news_sampled_test['news'], news_sampled_test['class'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End of iteration 1, added 1458 new labels.\n",
            "End of iteration 2, added 127 new labels.\n",
            "End of iteration 3, added 16 new labels.\n",
            "End of iteration 4, added 1 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.50      0.50        18\n",
            "           1       0.83      0.29      0.43        17\n",
            "           2       0.52      0.64      0.57        22\n",
            "           3       1.00      0.11      0.19        19\n",
            "           4       0.36      0.19      0.25        21\n",
            "           5       0.43      0.64      0.52        25\n",
            "           6       0.60      0.63      0.62        19\n",
            "           7       0.50      0.52      0.51        21\n",
            "           8       0.68      0.62      0.65        21\n",
            "           9       0.62      0.56      0.59        18\n",
            "          10       0.54      0.68      0.60        22\n",
            "          11       0.36      0.50      0.42        10\n",
            "          12       0.75      0.13      0.22        23\n",
            "          13       0.13      0.65      0.22        20\n",
            "          14       0.59      0.33      0.43        30\n",
            "          15       0.55      0.26      0.35        23\n",
            "          16       0.43      0.17      0.24        18\n",
            "          17       0.41      0.50      0.45        24\n",
            "          18       0.60      0.32      0.41        19\n",
            "          19       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           0.42       400\n",
            "   macro avg       0.52      0.41      0.41       400\n",
            "weighted avg       0.54      0.42      0.42       400\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}